{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFilter\n",
    "import ast\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5405bb3c52bd>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.3.1', True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 1920), (512, 1024))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(128 * 8, 128 * 15), (128 * 4, 128*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_root_folder = '/home/mvlab/Downloads/dataset/ena24/'\n",
    "path_ena_label = path_root_folder + 'ena24.json'\n",
    "path_ena_image = path_root_folder\n",
    "\n",
    "os.path.isdir(path_root_folder), os.path.isfile(path_ena_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padded_image_shape = (1024, 1920)\n",
    "min_stride = 128\n",
    "padded_image_shape = (128*4, 128*8)\n",
    "anchor_k = 9\n",
    "num_classes = 80#\n",
    "num_classes_real = 30\n",
    "max_data_m = 300000\n",
    "is_no_human = False\n",
    "use_size_down = True\n",
    "use_zoom_up_data = False\n",
    "use_ENA = False\n",
    "level_start = 3\n",
    "level_end = 8\n",
    "l1 = 1e-8\n",
    "activation = 'swish'#'selu' is not converted to tflite\n",
    "kernel_init = tf.initializers.he_normal()\n",
    "edgecolors = np.random.rand(num_classes, 3) \n",
    "edgecolors = np.minimum(edgecolors+0.1, 1.0)\n",
    "#class_names = ['bg', 'pig', 'crow']\n",
    "class_names_kor = ['배경', '모름', '돼지사체', '멧돼지사체', '돼지','멧돼지', '큰부리까마귀', '흰배지빠귀', '유리딱새', '동고비', \n",
    "               '박새', '호랑지빠귀', '검독수리', '너구리', '쥐', '다람쥐', '사람', \n",
    "                   'car' , '오소리', '개','고양이']\n",
    "class_names = ['bg', 'unknown', 'pig_dead', 'boar_dead', 'pig', 'boar', 'crow', 'bird_white', 'bird_ddak', 'bird_dong', \n",
    "               'bird_bak', 'bird_tire', 'bird_eagle', 'racoon', 'rat', 'squrrel', 'human', \n",
    "               'car', 'opossum', 'dog','cat']\n",
    "\n",
    "folder_pig = '/home/mvlab/Downloads/dataset/pig/'\n",
    "path_weight = \"retinanet/pig_EfficientDet-D2\"\n",
    "\n",
    "os.path.isdir(folder_pig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_ENA = [\n",
    "    {\"name\": \"Bird\",\"id\": 0},\n",
    "    {\"name\": \"Eastern Gray Squirrel\",\"id\": 1    },\n",
    "    {\"name\": \"Eastern Chipmunk\",      \"id\": 2    },\n",
    "    {\"name\": \"Woodchuck\",      \"id\": 3    },\n",
    "    {\"name\": \"Wild Turkey\",      \"id\": 4    },\n",
    "    {\"name\": \"White_Tailed_Deer\",      \"id\": 5    },\n",
    "    {\"name\": \"Virginia Opossum\",      \"id\": 6    },\n",
    "    {\"name\": \"Eastern Cottontail\",      \"id\": 7    },\n",
    "    {\"name\": \"Human\",      \"id\": 8    },\n",
    "    {\"name\": \"Vehicle\",      \"id\": 9    },\n",
    "    {\"name\": \"Striped Skunk\",      \"id\": 10    },\n",
    "    {\"name\": \"Red Fox\",      \"id\": 11    },\n",
    "    {\"name\": \"Eastern Fox Squirrel\",      \"id\": 12    },\n",
    "    {\"name\": \"Northern Raccoon\",      \"id\": 13    },\n",
    "    {\"name\": \"Grey Fox\",      \"id\": 14    },\n",
    "    {\"name\": \"Horse\",      \"id\": 15    },\n",
    "    {\"name\": \"Dog\",      \"id\": 16    },\n",
    "    {\"name\": \"American Crow\",      \"id\": 17    },\n",
    "    {\"name\": \"Chicken\",      \"id\": 18    },\n",
    "    {\"name\": \"Domestic Cat\",      \"id\": 19    },\n",
    "    {\"name\": \"Coyote\",      \"id\": 20    },\n",
    "    {\"name\": \"Bobcat\",      \"id\": 21    },\n",
    "    {\"name\": \"American Black Bear\",      \"id\": 22    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_ENA = len(categories_ENA)\n",
    "num_classes_ENA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_dict = {\n",
    "    1: class_names.index('squrrel'), \n",
    "    2: class_names.index('squrrel'), \n",
    "    6: class_names.index('opossum'),\n",
    "    8: class_names.index('human'),\n",
    "    9: class_names.index('car'),\n",
    "    12: class_names.index('squrrel'),\n",
    "    13: class_names.index('racoon'),\n",
    "    16: class_names.index('dog'),\n",
    "    17: class_names.index('crow'),    \n",
    "    19: class_names.index('cat'),\n",
    "    20: class_names.index('dog'),\n",
    "    21: class_names.index('cat')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9676 {'id': '1', 'file_name': '1.jpg', 'width': 1920, 'height': 1080}\n",
      "11596 {'id': 'd8e94bd2-1df9-11ea-8572-5cf370671a19', 'image_id': '1', 'category_id': 0, 'bbox': [5.47008, 974.4170399999999, 162.279168, 72.97300800000001]}\n",
      "9676\n"
     ]
    }
   ],
   "source": [
    "with open(path_ena_label, 'r') as j:\n",
    "     contents = json.loads(j.read())\n",
    "\n",
    "images = contents['images']\n",
    "annotations = contents['annotations']\n",
    "print(len(images), images[0])\n",
    "print(len(annotations), annotations[0])\n",
    "\n",
    "file_name_id_dict = dict()\n",
    "for image in images:\n",
    "    file_name = image['file_name']\n",
    "    id_num = image['id']    \n",
    "    file_name_id_dict[file_name] = id_num\n",
    "print(len(file_name_id_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9676, 9676, '1', '999')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_values = file_name_id_dict.values()\n",
    "len(id_values), len(set(id_values)), min(id_values), max(id_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 23)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(categories_ENA), len(categories_ENA)#, categories_ENA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 15,\n",
       " 2: 15,\n",
       " 6: 18,\n",
       " 8: 16,\n",
       " 9: 17,\n",
       " 12: 15,\n",
       " 13: 13,\n",
       " 16: 19,\n",
       " 17: 6,\n",
       " 19: 20,\n",
       " 20: 19,\n",
       " 21: 20}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.array((1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9676, 1920)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_bbox_dict = dict()\n",
    "multi_object_k = 0\n",
    "for annotation in annotations:    \n",
    "    image_id = annotation['image_id']\n",
    "    bbox = annotation['bbox']\n",
    "    category_id = annotation['category_id']    \n",
    "    image_id = image_id\n",
    "           \n",
    "    bbox = np.array(bbox, dtype=np.int)\n",
    "    x, y, w, h = bbox\n",
    "    #categories_ENA\n",
    "    #print(category_id)\n",
    "    category_id = int(category_id)\n",
    "    \n",
    "    if category_id in change_dict.keys():\n",
    "        #print(category_id, '>', change_dict[category_id])\n",
    "        category_id = change_dict[category_id]        \n",
    "    else:        \n",
    "        category_id = class_names.index('unknown')#unknown        \n",
    "    \n",
    "    bbox = [category_id, x, y, x+w, y+h]\n",
    "    #print('image_id',image_id, 'category',category_id,'bbox', bbox)\n",
    "    \n",
    "    if image_id in id_bbox_dict.keys():\n",
    "        id_bbox_dict[image_id].extend(bbox)        \n",
    "        multi_object_k += 1\n",
    "    else:\n",
    "        id_bbox_dict[image_id] = bbox\n",
    "        \n",
    "len(id_bbox_dict.keys()), multi_object_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8789, dict)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation = dict()\n",
    "for file_name in file_name_id_dict.keys():\n",
    "    file_full_path = path_ena_image + file_name\n",
    "        \n",
    "    file_id = file_name_id_dict[file_name]\n",
    "    bboxes = id_bbox_dict[file_id]\n",
    "    bboxes_2d = np.reshape(bboxes, [-1, 5])\n",
    "    \n",
    "    if os.path.isfile(file_full_path):        \n",
    "        annotation[file_full_path] = bboxes_2d\n",
    "    else:\n",
    "        continue\n",
    "len(annotation), type(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_data_m, use_size_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100//2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_list 0\n"
     ]
    }
   ],
   "source": [
    "input_list = []\n",
    "bbox_list = []\n",
    "\n",
    "i = -1\n",
    "for file_name in file_name_id_dict.keys():\n",
    "    i += 1\n",
    "    \n",
    "    if not use_ENA: \n",
    "        break\n",
    "    #if i%2 != 0 : continue\n",
    "    \n",
    "    file_full_path_train = path_ena_image + file_name\n",
    "        \n",
    "    file_id = file_name_id_dict[file_name]\n",
    "    bboxes = id_bbox_dict[file_id]\n",
    "    bboxes_2d = np.reshape(bboxes, [-1, 5])\n",
    "    cls = bboxes_2d[:, :1]\n",
    "    boxes_2d = bboxes_2d[:, 1:]\n",
    "        \n",
    "    if os.path.isfile(file_full_path_train):\n",
    "        try:\n",
    "            img = Image.open(file_full_path_train)\n",
    "            w, h = img.size\n",
    "            if i==0:print('img size', img.size)\n",
    "            if use_size_down:                \n",
    "                img = img.resize((w//3, h//2))\n",
    "                                \n",
    "            img_arr = np.array(img)\n",
    "            \n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            box_norm = boxes_2d.astype(np.float) / scale.astype(np.float)        \n",
    "            \n",
    "            cls_bbox_norm = np.concatenate((cls, box_norm), axis=1)\n",
    "            \n",
    "            input_list.append(img_arr)\n",
    "            bbox_list.append(cls_bbox_norm)    \n",
    "            if len(input_list)%200==0:\n",
    "                print('reading image', len(file_name_id_dict.keys()), i, len(input_list))        \n",
    "            if len(input_list)>=max_data_m:\n",
    "                break\n",
    "        except:\n",
    "            print('error', file_full_path_train)\n",
    "            continue\n",
    "    else:\n",
    "        #print('not exist', file_full_path_train)\n",
    "        continue\n",
    "print('input_list', len(input_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_list 0 0\n"
     ]
    }
   ],
   "source": [
    "print('input_list', len(input_list), len(bbox_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46200, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_2021-1-2.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_attributes</th>\n",
       "      <th>region_count</th>\n",
       "      <th>region_id</th>\n",
       "      <th>region_shape_attributes</th>\n",
       "      <th>region_attributes</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>geomex-pig01_2020-09-03_125416_temp-19.70_wat-...</td>\n",
       "      <td>362254</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":583,\"y\":463,\"width\":453,\"he...</td>\n",
       "      <td>돼지사체</td>\n",
       "      <td>포유류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>geomex-pig01_2020-09-03_125416_temp-19.70_wat-...</td>\n",
       "      <td>362254</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":560,\"y\":563,\"width\":224,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>geomex-pig01_2020-09-03_125706_temp-19.80_wat-...</td>\n",
       "      <td>365469</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":907,\"y\":332,\"width\":211,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>geomex-pig01_2020-09-03_125706_temp-19.80_wat-...</td>\n",
       "      <td>365469</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":584,\"y\":457,\"width\":369,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>geomex-pig01_2020-09-03_125956_temp-20.70_wat-...</td>\n",
       "      <td>362131</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":339,\"y\":128,\"width\":107,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46195</th>\n",
       "      <td>89582</td>\n",
       "      <td>geomex-pig06_2020-11-12_143819_temp-16.00_wat-...</td>\n",
       "      <td>494167</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":1011,\"y\":160,\"width\":91,\"he...</td>\n",
       "      <td>큰부리까마귀</td>\n",
       "      <td>조류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46196</th>\n",
       "      <td>89583</td>\n",
       "      <td>geomex-pig06_2020-11-12_143819_temp-16.00_wat-...</td>\n",
       "      <td>494167</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":999,\"y\":100,\"width\":84,\"hei...</td>\n",
       "      <td>큰부리까마귀</td>\n",
       "      <td>조류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46197</th>\n",
       "      <td>89584</td>\n",
       "      <td>geomex-pig06_2020-11-12_143819_temp-16.00_wat-...</td>\n",
       "      <td>494167</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":623,\"y\":627,\"width\":1035,\"h...</td>\n",
       "      <td>멧돼지사체</td>\n",
       "      <td>포유류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46198</th>\n",
       "      <td>89585</td>\n",
       "      <td>geomex-pig06_2020-11-12_143819_temp-16.00_wat-...</td>\n",
       "      <td>494167</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":812,\"y\":340,\"width\":795,\"he...</td>\n",
       "      <td>멧돼지사체</td>\n",
       "      <td>포유류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46199</th>\n",
       "      <td>89586</td>\n",
       "      <td>geomex-pig06_2020-11-12_143819_temp-16.00_wat-...</td>\n",
       "      <td>494167</td>\n",
       "      <td>{}</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":989,\"y\":111,\"width\":639,\"he...</td>\n",
       "      <td>멧돼지사체</td>\n",
       "      <td>포유류</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46200 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           filename  \\\n",
       "0               0  geomex-pig01_2020-09-03_125416_temp-19.70_wat-...   \n",
       "1               1  geomex-pig01_2020-09-03_125416_temp-19.70_wat-...   \n",
       "2               2  geomex-pig01_2020-09-03_125706_temp-19.80_wat-...   \n",
       "3               3  geomex-pig01_2020-09-03_125706_temp-19.80_wat-...   \n",
       "4               4  geomex-pig01_2020-09-03_125956_temp-20.70_wat-...   \n",
       "...           ...                                                ...   \n",
       "46195       89582  geomex-pig06_2020-11-12_143819_temp-16.00_wat-...   \n",
       "46196       89583  geomex-pig06_2020-11-12_143819_temp-16.00_wat-...   \n",
       "46197       89584  geomex-pig06_2020-11-12_143819_temp-16.00_wat-...   \n",
       "46198       89585  geomex-pig06_2020-11-12_143819_temp-16.00_wat-...   \n",
       "46199       89586  geomex-pig06_2020-11-12_143819_temp-16.00_wat-...   \n",
       "\n",
       "       file_size file_attributes  region_count  region_id  \\\n",
       "0         362254              {}             2          0   \n",
       "1         362254              {}             2          1   \n",
       "2         365469              {}             2          0   \n",
       "3         365469              {}             2          1   \n",
       "4         362131              {}             2          0   \n",
       "...          ...             ...           ...        ...   \n",
       "46195     494167              {}            15         10   \n",
       "46196     494167              {}            15         11   \n",
       "46197     494167              {}            15         12   \n",
       "46198     494167              {}            15         13   \n",
       "46199     494167              {}            15         14   \n",
       "\n",
       "                                 region_shape_attributes region_attributes  \\\n",
       "0      {\"name\":\"rect\",\"x\":583,\"y\":463,\"width\":453,\"he...              돼지사체   \n",
       "1      {\"name\":\"rect\",\"x\":560,\"y\":563,\"width\":224,\"he...                사람   \n",
       "2      {\"name\":\"rect\",\"x\":907,\"y\":332,\"width\":211,\"he...                사람   \n",
       "3      {\"name\":\"rect\",\"x\":584,\"y\":457,\"width\":369,\"he...                사람   \n",
       "4      {\"name\":\"rect\",\"x\":339,\"y\":128,\"width\":107,\"he...                사람   \n",
       "...                                                  ...               ...   \n",
       "46195  {\"name\":\"rect\",\"x\":1011,\"y\":160,\"width\":91,\"he...            큰부리까마귀   \n",
       "46196  {\"name\":\"rect\",\"x\":999,\"y\":100,\"width\":84,\"hei...            큰부리까마귀   \n",
       "46197  {\"name\":\"rect\",\"x\":623,\"y\":627,\"width\":1035,\"h...             멧돼지사체   \n",
       "46198  {\"name\":\"rect\",\"x\":812,\"y\":340,\"width\":795,\"he...             멧돼지사체   \n",
       "46199  {\"name\":\"rect\",\"x\":989,\"y\":111,\"width\":639,\"he...             멧돼지사체   \n",
       "\n",
       "      category  \n",
       "0          포유류  \n",
       "1          영장류  \n",
       "2          영장류  \n",
       "3          영장류  \n",
       "4          영장류  \n",
       "...        ...  \n",
       "46195       조류  \n",
       "46196       조류  \n",
       "46197      포유류  \n",
       "46198      포유류  \n",
       "46199      포유류  \n",
       "\n",
       "[46200 rows x 9 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188539,\n",
       " '/home/mvlab/Downloads/dataset/pig/pig2/2020-10/geomex-pig02_2020-09-10_074418_temp-18.00_wat-97.10.jpg')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_images_0 = glob(folder_pig + '*/*/*.jpg')\n",
    "path_images_1 = glob(folder_pig + '*/*/*/*.jpg')\n",
    "path_images = path_images_0 + path_images_1\n",
    "\n",
    "dict_path = dict() #5.0 GiB 7.9%\n",
    "for path_image in path_images:\n",
    "    image_file_name = path_image.split(os.sep)[-1]\n",
    "    #print('image_file_name', image_file_name)\n",
    "    dict_path[image_file_name] = path_image    \n",
    "\n",
    "len(dict_path)\n",
    "len(path_images), path_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_size</th>\n",
       "      <th>file_attributes</th>\n",
       "      <th>region_count</th>\n",
       "      <th>region_id</th>\n",
       "      <th>region_shape_attributes</th>\n",
       "      <th>region_attributes</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>geomex-pig01_2020-09-03_125416_temp-19.70_wat-...</td>\n",
       "      <td>362254</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":583,\"y\":463,\"width\":453,\"he...</td>\n",
       "      <td>돼지사체</td>\n",
       "      <td>포유류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>geomex-pig01_2020-09-03_125416_temp-19.70_wat-...</td>\n",
       "      <td>362254</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":560,\"y\":563,\"width\":224,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>geomex-pig01_2020-09-03_125706_temp-19.80_wat-...</td>\n",
       "      <td>365469</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":907,\"y\":332,\"width\":211,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>geomex-pig01_2020-09-03_125706_temp-19.80_wat-...</td>\n",
       "      <td>365469</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":584,\"y\":457,\"width\":369,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>geomex-pig01_2020-09-03_125956_temp-20.70_wat-...</td>\n",
       "      <td>362131</td>\n",
       "      <td>{}</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>{\"name\":\"rect\",\"x\":339,\"y\":128,\"width\":107,\"he...</td>\n",
       "      <td>사람</td>\n",
       "      <td>영장류</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           filename  file_size  \\\n",
       "0           0  geomex-pig01_2020-09-03_125416_temp-19.70_wat-...     362254   \n",
       "1           1  geomex-pig01_2020-09-03_125416_temp-19.70_wat-...     362254   \n",
       "2           2  geomex-pig01_2020-09-03_125706_temp-19.80_wat-...     365469   \n",
       "3           3  geomex-pig01_2020-09-03_125706_temp-19.80_wat-...     365469   \n",
       "4           4  geomex-pig01_2020-09-03_125956_temp-20.70_wat-...     362131   \n",
       "\n",
       "  file_attributes  region_count  region_id  \\\n",
       "0              {}             2          0   \n",
       "1              {}             2          1   \n",
       "2              {}             2          0   \n",
       "3              {}             2          1   \n",
       "4              {}             2          0   \n",
       "\n",
       "                             region_shape_attributes region_attributes  \\\n",
       "0  {\"name\":\"rect\",\"x\":583,\"y\":463,\"width\":453,\"he...              돼지사체   \n",
       "1  {\"name\":\"rect\",\"x\":560,\"y\":563,\"width\":224,\"he...                사람   \n",
       "2  {\"name\":\"rect\",\"x\":907,\"y\":332,\"width\":211,\"he...                사람   \n",
       "3  {\"name\":\"rect\",\"x\":584,\"y\":457,\"width\":369,\"he...                사람   \n",
       "4  {\"name\":\"rect\",\"x\":339,\"y\":128,\"width\":107,\"he...                사람   \n",
       "\n",
       "  category  \n",
       "0      포유류  \n",
       "1      영장류  \n",
       "2      영장류  \n",
       "3      영장류  \n",
       "4      영장류  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no full path geomex-pig03_2020-09-03_142459_temp-20.10_wat-77.90.jpg 돼지사체\n",
      "no full path geomex-pig03_2020-09-03_142459_temp-20.10_wat-77.90.jpg 사람\n"
     ]
    }
   ],
   "source": [
    "annotation = dict()\n",
    "object_width_list = []\n",
    "\n",
    "ind = np.arange(len(df))\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i in ind:\n",
    "    \n",
    "    sample_row = df.iloc[i]\n",
    "    filename = sample_row['filename']    \n",
    "    region_id = sample_row['region_id']\n",
    "    box = sample_row['region_shape_attributes']    \n",
    "    region_attributes = sample_row['region_attributes']    \n",
    "    \n",
    "    if filename not in dict_path.keys():\n",
    "        print('no full path', filename, region_attributes)\n",
    "        continue\n",
    "    full_path = dict_path[filename]\n",
    "    \n",
    "    try:\n",
    "        box_map = ast.literal_eval(box)            \n",
    "    except:\n",
    "        print('except filename',i, filename)\n",
    "        print('box', box)\n",
    "        print('region_attributes', region_attributes)\n",
    "        continue\n",
    "            \n",
    "    cls = region_attributes\n",
    "    if cls in class_names_kor:\n",
    "        cls_num = class_names_kor.index(cls)    \n",
    "    else:\n",
    "        print('cls unknown', cls)\n",
    "        print('filename', filename)\n",
    "        continue\n",
    "                \n",
    "    x0 = box_map['x']\n",
    "    y0 = box_map['y']\n",
    "    width = box_map['width']\n",
    "    height = box_map['height']    \n",
    "   \n",
    "    object_width_list.append(width)    \n",
    "    bbox = [cls_num, x0, y0, x0 + width, y0 + height]\n",
    "    \n",
    "    if full_path not in annotation.keys():\n",
    "        annotation[full_path] = []\n",
    "    \n",
    "    annotation[full_path].append(bbox)            \n",
    "    #print(i, df.loc[i, 'filename'], width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13164"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation)#, annotation.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  3  3]\n",
      "[ 2 16  2  2]\n",
      "[16]\n",
      "[ 3  3 16]\n",
      "[ 3 16  3  3]\n",
      "[16 16  3]\n",
      "[16  1  2 16  2  1 16  1  2  2  1 16]\n",
      "[ 3 16]\n",
      "[16 16  2 16 16  2]\n",
      "[ 2  2  1  1  1  2  1 16 16 16  2 16]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 1 16 16  1  1 16 16  1]\n",
      "[16  2  1  2  1 16 16  1 16  1  2  2]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[ 2 16  2 16  2 16 16  2]\n",
      "[ 3  3 16  3]\n",
      "[ 2  2  2 16]\n",
      "[16  3  3]\n",
      "[16  3]\n",
      "[ 3  3 16]\n",
      "[ 3 16 16]\n",
      "[ 3  3  3 16 16]\n",
      "[ 1 16 16  1  1]\n",
      "[16 16 16 16]\n",
      "[16  2]\n",
      "[ 3 16]\n",
      "[ 2  2  2 16]\n",
      "[16  2  2]\n",
      "[16  3]\n",
      "[ 2  2  2 16]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[ 2 16  2  2]\n",
      "[ 3  3 16]\n",
      "[ 3  3 16  3]\n",
      "[ 2 16  2  2]\n",
      "[16  2  2 16 16 16  2  2]\n",
      "[ 3 16]\n",
      "[16 16 16]\n",
      "[ 3  3 16]\n",
      "[16 16  3]\n",
      "[16 16  3]\n",
      "[16 16 16 16]\n",
      "[16]\n",
      "[ 3 16  3]\n",
      "[ 1  1 16 16  1  1 16 16]\n",
      "[ 2  2 16  2]\n",
      "[16  3]\n",
      "[ 2  2  2 16]\n",
      "[ 2 16]\n",
      "[16  2  2  2]\n",
      "[ 2  2 16  2]\n",
      "[16  2]\n",
      "[ 3  3 16]\n",
      "[ 1  1  1  1  1  1  1 16 16 16 16  1  1  1  1  1]\n",
      "[ 2  2 16  2]\n",
      "[16  1  1  1 16  2  1 16 16  2  2  2]\n",
      "[ 3  3 16  3]\n",
      "[16]\n",
      "[ 2 16 16  2 16  2 16  2]\n",
      "[16 16 16 16]\n",
      "[ 2 16  2  2]\n",
      "[ 2 16]\n",
      "[16 16  3]\n",
      "[ 3  3 16]\n",
      "[ 3 16]\n",
      "[ 3  3  3 16]\n",
      "[16]\n",
      "[ 3 16  3]\n",
      "[ 2  2 16  2]\n",
      "[ 2  2 16  2]\n",
      "[16  2  2  2 16 16  2 16]\n",
      "[ 3 16  3  3]\n",
      "[16  3  3]\n",
      "[ 3 16  3  3]\n",
      "[ 3 16 16  3  3]\n",
      "[16  3]\n",
      "[ 2  2 16  2]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3 16  3 16]\n",
      "[16  3  3]\n",
      "[ 3  3  3 16]\n",
      "[ 2  2 16  2]\n",
      "[ 3  3 16]\n",
      "[ 3  3 16]\n",
      "[ 3 16  3  3]\n",
      "[ 3 16  3  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 1  1  1  1 16  1  1 16  1 16  1 16]\n",
      "[16  3]\n",
      "[ 2 16  2  2]\n",
      "[ 2  2  2 16]\n",
      "[ 3  3 16]\n",
      "[ 2  2 16  2]\n",
      "[ 2  2 16  2 16 16  2 16]\n",
      "[ 1 16  1  1  2  1  1  2 16  2  1  1  2 16  1 16]\n",
      "[16]\n",
      "[ 2  2 16  2]\n",
      "[16  3 16]\n",
      "[16  3  3]\n",
      "[ 3 16 16]\n",
      "[ 3 16 16]\n",
      "[16 16  3]\n",
      "[ 3 16  3  3]\n",
      "[16  2  2  2]\n",
      "[16]\n",
      "[ 1 16  2]\n",
      "[ 3 16]\n",
      "[16 16  3 16  3 16 16]\n",
      "[16  3]\n",
      "[ 2  2 16  2]\n",
      "[16  3]\n",
      "[ 3 16 16]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 2  2 16  2]\n",
      "[16  3]\n",
      "[ 3 16  3  3]\n",
      "[16  3]\n",
      "[16  1  1 16 16  1 16  1]\n",
      "[ 2 16]\n",
      "[ 2 16  2  2]\n",
      "[16  3]\n",
      "[16  3 16]\n",
      "[ 2  2  2 16]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[ 3 16 16]\n",
      "[ 3 16 16]\n",
      "[16  3]\n",
      "[ 2 16  2  2]\n",
      "[16  3  3]\n",
      "[16  3  3 16 16]\n",
      "[ 1 16  1 16  1 16 16  1]\n",
      "[ 2 16]\n",
      "[ 1  2  1  2 16  1 16 16  2  1 16  2]\n",
      "[ 3 16  3]\n",
      "[ 2  2 16  2]\n",
      "[16  2]\n",
      "[ 2 16]\n",
      "[ 3 16]\n",
      "[ 2 16 16]\n",
      "[16  3 16]\n",
      "[16  2  2  2]\n",
      "[ 2 16  1  2  1  2  1 16  2  1 16 16]\n",
      "[16 16  3]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3 16  3]\n",
      "[ 2  2  2 16]\n",
      "[16 16 16 16]\n",
      "[ 3  3  3 16]\n",
      "[16  3  3  3]\n",
      "[16]\n",
      "[ 3  3  3 16]\n",
      "[16  3 16]\n",
      "[16 16  3]\n",
      "[16 16]\n",
      "[16  6 16  3]\n",
      "[ 3 16  3]\n",
      "[16  3 16]\n",
      "[16]\n",
      "[ 2  2 16  2]\n",
      "[ 3  3  3 16]\n",
      "[ 2  2 16  2]\n",
      "[ 3 16]\n",
      "[ 2  2 16  2]\n",
      "[ 2 16]\n",
      "[ 3 16 16]\n",
      "[16 16  2  1  1  1 16  2 16  2  1  2]\n",
      "[16  2  2  2]\n",
      "[ 3 16]\n",
      "[ 2  2 16  2]\n",
      "[ 3 16]\n",
      "[16  2]\n",
      "[ 1 16  1  2]\n",
      "[16]\n",
      "[16  3  3]\n",
      "[ 3 16]\n",
      "[16  2]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 2  2 16  2]\n",
      "[ 2  2 16  2]\n",
      "[ 3 16 16]\n",
      "[16  3]\n",
      "[16 16  3]\n",
      "[16  2 16 16 16  2  2  2]\n",
      "[ 3 16]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[ 2 16  2  2]\n",
      "[ 3  3 16  3]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[ 3 16 16]\n",
      "[16  3  3  3]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[16]\n",
      "[16  3]\n",
      "[16  3  3 16]\n",
      "[ 3  3 16 16]\n",
      "[ 2 16  2 16  2  2 16 16]\n",
      "[ 3 16]\n",
      "[ 3 16  3  3]\n",
      "[16]\n",
      "[ 3 16 16]\n",
      "[16 16  3]\n",
      "[ 2 16  2  2]\n",
      "[16  3]\n",
      "[ 1 16  1  2]\n",
      "[16  2 16]\n",
      "[ 3 16  3  3]\n",
      "[16  3]\n",
      "[ 3 16  3]\n",
      "[ 3 16]\n",
      "[16  3 16 16]\n",
      "[ 3 16]\n",
      "[16 16  3  3]\n",
      "[16  3]\n",
      "[ 2 16  1  2 16 16  2  1  2  1 16  1]\n",
      "[16  3]\n",
      "[ 3  3 16  3]\n",
      "[ 3 16]\n",
      "[ 3  3 16]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16 16]\n",
      "[ 3 16  3]\n",
      "[ 2 16  2  2]\n",
      "[ 3 16  3]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16 16  3  3]\n",
      "[16 16  3]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16  1]\n",
      "[16]\n",
      "[ 2  2 16  2]\n",
      "[16  3  3  3]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 3  3 16  3]\n",
      "[16 16]\n",
      "[16]\n",
      "[ 3  3 16  3]\n",
      "[ 3 16  3]\n",
      "[ 3  3  3 16]\n",
      "[16  3]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[ 3 16  3]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3  3 16]\n",
      "[ 2 16  2  2]\n",
      "[16  3]\n",
      "[16  2]\n",
      "[ 3 16]\n",
      "[ 2 16 16 16  2 16  2  2]\n",
      "[16 16]\n",
      "[16  3  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 3  3  3 16]\n",
      "[ 3 16 16  3]\n",
      "[16]\n",
      "[16 16  3]\n",
      "[ 1 16  1  1 16  1 16  1  1  1 16  1]\n",
      "[16 16  1 16  1  1 16  1  2  2  2  2]\n",
      "[16]\n",
      "[ 3 16  3 16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[16  2]\n",
      "[16]\n",
      "[ 3 16  3  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[16  3  3]\n",
      "[ 3 16  3  3]\n",
      "[ 2  2  2 16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[ 3  1 16]\n",
      "[ 3  3 16]\n",
      "[ 3 16]\n",
      "[16  3 16  3]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 3 16  3]\n",
      "[16  3 16]\n",
      "[ 3  3 16]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[ 2  2 16 16 16  2 16  2]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16 16]\n",
      "[16]\n",
      "[16  2]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[ 2 16]\n",
      "[16  3]\n",
      "[ 3  3 16]\n",
      "[16  3  3]\n",
      "[ 1 16  2]\n",
      "[ 2 16]\n",
      "[ 3 16]\n",
      "[ 2  2 16  2]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[ 3  3 16]\n",
      "[16  3]\n",
      "[ 2  2 16  2]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16  3  3]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16  1]\n",
      "[16  3  3]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 3  3 16  3]\n",
      "[ 3  3 16]\n",
      "[ 3 16]\n",
      "[ 2 16]\n",
      "[16  2  2  2]\n",
      "[ 3  1 16  3]\n",
      "[ 3 16]\n",
      "[ 3  3 16 16]\n",
      "[16  3]\n",
      "[ 2 16  2  2]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[16  3]\n",
      "[ 2  2  2 16]\n",
      "[ 3  3 16]\n",
      "[16]\n",
      "[ 3 16 16]\n",
      "[ 3 16 16]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[16]\n",
      "[16]\n",
      "[16  1 16  3  3]\n",
      "[ 3 16]\n",
      "[ 3 16  3]\n",
      "[16  1  2]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 3 16  3  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[16 16]\n",
      "[ 3  3 16  3]\n",
      "[16 16  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[16 16]\n",
      "[16  2  2  2]\n",
      "[16  3  3]\n",
      "[ 3  3 16]\n",
      "[16]\n",
      "[ 3  3 16  3]\n",
      "[ 3 16]\n",
      "[ 3  3 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16]\n",
      "[ 3 16 16]\n",
      "[16]\n",
      "[16 16  3]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16  2  2  2]\n",
      "[16]\n",
      "[16]\n",
      "[16  2  2  2]\n",
      "[ 3 16]\n",
      "[16 16  3  3]\n",
      "[ 2  2 16  2]\n",
      "[16]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[ 3 16 16]\n",
      "[16]\n",
      "[16]\n",
      "[ 3 16  3]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16  3]\n",
      "[ 3  3  3 16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16 16  3  3]\n",
      "[16  3  3]\n",
      "[ 2 16]\n",
      "[ 3 16  3]\n",
      "[16 16]\n",
      "[ 3 16]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[16  3  3]\n",
      "[ 3 16]\n",
      "[16 16  3]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[ 2  2  2 16]\n",
      "[ 3 16]\n",
      "[16  2]\n",
      "[16]\n",
      "[16 16  3 16]\n",
      "[16  3]\n",
      "[16  3  3]\n",
      "[ 3 16 16]\n",
      "[16  2]\n",
      "[16]\n",
      "[ 3  3 16]\n",
      "[16  3]\n",
      "[16]\n",
      "[16  2]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[16 16]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[ 2  2 16  2]\n",
      "[ 3 16]\n",
      "[ 2 16]\n",
      "[16  3]\n",
      "[16  2]\n",
      "[16  2]\n",
      "[ 3  3 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16  3  3]\n",
      "[16  2  2  2]\n",
      "[16  3]\n",
      "[16]\n",
      "[ 2 16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 2  2  2 16]\n",
      "[ 2  2 16  2]\n",
      "[16 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 2 16]\n",
      "[16]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[16 16  3]\n",
      "[16  2 16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[16 16  3]\n",
      "[16]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[ 2 16]\n",
      "[ 3 16  3]\n",
      "[16 16]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16]\n",
      "[16]\n",
      "[ 3  3 16]\n",
      "[16 16]\n",
      "[ 2 16]\n",
      "[ 2 16]\n",
      "[ 3 16]\n",
      "[ 3 16]\n",
      "[ 2 16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[16  3]\n",
      "[16]\n",
      "[16]\n",
      "[16  3]\n",
      "[ 2 16]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16  3]\n",
      "[16 16]\n",
      "[16  2]\n",
      "[16  3]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16]\n",
      "[16]\n",
      "[16]\n",
      "[ 3 16]\n",
      "[16]\n",
      "[16]\n",
      "[16  3]\n",
      "[16]\n",
      "[16]\n",
      "[16]\n",
      "[16]\n",
      "[16]\n"
     ]
    }
   ],
   "source": [
    "annotation_no_human = annotation.copy()\n",
    "for key in annotation.keys():\n",
    "    cbox = np.stack(annotation[key])\n",
    "    if np.max(cbox[:, 0]) >= class_names_kor.index('사람'):           \n",
    "        print(cbox[:, 0])\n",
    "        del annotation_no_human[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13164, 12611)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation), len(annotation_no_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_no_human:\n",
    "    annotation = annotation_no_human.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 256)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**level_start, 2**level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 329.7646867829776, 1871)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(object_width_list), np.mean(object_width_list), np.max(object_width_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('width '+ str(np.mean(object_width_list)))\n",
    "ax = plt.hist(object_width_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13164 200 200\n",
      "13164 400 400\n",
      "13164 600 600\n",
      "13164 800 800\n",
      "13164 1000 1000\n",
      "13164 1200 1200\n",
      "13164 1400 1400\n",
      "13164 1600 1600\n",
      "13164 1800 1800\n",
      "13164 2000 2000\n",
      "13164 2200 2200\n",
      "13164 2400 2400\n",
      "13164 2600 2600\n",
      "13164 2800 2800\n",
      "13164 3000 3000\n",
      "13164 3200 3200\n",
      "13164 3400 3400\n",
      "13164 3600 3600\n"
     ]
    }
   ],
   "source": [
    "def load_pig():\n",
    "    input_list = []\n",
    "    bbox_list = []\n",
    "    path_list = []\n",
    "    for key in annotation:\n",
    "        cls_bbox = annotation[key]\n",
    "        cls_bbox = np.array(cls_bbox).reshape([-1, 5])\n",
    "        cls = cls_bbox[:, 0:1]\n",
    "        bbox = np.array(cls_bbox[:, 1:])\n",
    "\n",
    "        path_image = key\n",
    "        \n",
    "        img = Image.open(path_image)    \n",
    "        if use_size_down:\n",
    "            w, h = img.size\n",
    "            img = img.resize((w//2, h//2))\n",
    "\n",
    "        scale = np.array((w, h, w, h))\n",
    "        scale = np.reshape(scale, (1, 4))       \n",
    "        #print(key, cls, cls_bbox.dtype, cls_bbox, 'wh',img.width, img.height)\n",
    "        img_arr = np.array(img)    \n",
    "        bbox_norm = bbox.astype(np.float) / scale.astype(np.float)\n",
    "        cls = np.array(cls).reshape((-1, 1))\n",
    "        cls_bbox_norm = np.concatenate((cls, bbox_norm), axis=1)\n",
    "\n",
    "        input_list.append(img_arr)\n",
    "        bbox_list.append(cls_bbox_norm)\n",
    "        path_list.append(key)\n",
    "        if len(input_list) % 200 == 0:        \n",
    "            print(len(annotation), len(input_list), len(bbox_list))   \n",
    "        if len(input_list) > max_data_m:\n",
    "            break\n",
    "        \n",
    "    print(len(input_list), len(bbox_list))\n",
    "    return input_list, bbox_list\n",
    "    \n",
    "input_list_pig, bbox_list_pig = load_pig()#1709, 1759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(class_names_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbbox = np.concatenate(bbox_list_pig, 0)\n",
    "print(cbbox.shape)\n",
    "h = plt.hist(cbbox[:, 0], bins=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_bbox_image(image, boxes):\n",
    "    img_objects = []\n",
    "    image = np.array(image)\n",
    "    for box in boxes:        \n",
    "        box = box.astype(np.int)\n",
    "        x1, y1, x2, y2 = box\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1        \n",
    "        crop_image_arr = image[y1:y2, x1:x2]\n",
    "        ch, cw, cc = crop_image_arr.shape\n",
    "        if ch>1 and cw>1:\n",
    "            img_objects.append(crop_image_arr)\n",
    "        else:\n",
    "            print('crop_bbox_image', x2-x1, y2-y1, 'crop_image_arr.shape', crop_image_arr.shape)\n",
    "        \n",
    "    return img_objects\n",
    "    \n",
    "    \n",
    "def attach_crop_image(image, boxes, max_crop=200):\n",
    "        \n",
    "    crop_bbox_arr = crop_bbox_image(image, np.array(boxes)[:max_crop])\n",
    "    bbox_k = len(crop_bbox_arr)\n",
    "    max_col = 30\n",
    "    \n",
    "    if bbox_k > 0:\n",
    "        img_h, img_w, img_c = image.shape\n",
    "        object_img_w = img_w//bbox_k        \n",
    "        resize_h = img_h // 8\n",
    "        resize_w = img_w // bbox_k  \n",
    "        resize_w = min(max(resize_w, img_w//max_col), img_w//8)\n",
    "        \n",
    "        footer_h = resize_h * (1 + (bbox_k-1)//max_col)\n",
    "        footer = np.zeros((footer_h, img_w, img_c), np.uint8)\n",
    "        \n",
    "        for i in range(min(bbox_k, max_crop)):\n",
    "            crop_arr = crop_bbox_arr[i]\n",
    "            crop_img = Image.fromarray(crop_arr)                \n",
    "            crop_img = crop_img.resize((resize_w, resize_h))\n",
    "            crop_arr_resized = np.array(crop_img)\n",
    "            offset_y = (i//max_col) * resize_h\n",
    "            offset_x = (i%max_col) * resize_w\n",
    "            footer[offset_y:offset_y+resize_h, offset_x:offset_x+resize_w] = crop_arr_resized\n",
    "\n",
    "        seperate_line = np.zeros_like(footer[:2])\n",
    "        image = np.concatenate((image, seperate_line, footer), axis=0)    \n",
    "    return image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detections_simple(\n",
    "    image, boxes, classes, figsize=(12, 12), linewidth=1, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)    \n",
    "    \n",
    "    img_h, img_w, img_c = image.shape\n",
    "    \n",
    "    image = attach_crop_image(image, boxes, max_crop=100)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "    boxes_width = boxes[:, 2] - boxes[:, 0]\n",
    "    boxes_height = boxes[:, 3] - boxes[:, 1]\n",
    "    box_min_width = np.min(boxes_width)\n",
    "    box_max_width = np.max(boxes_width)\n",
    "    title = str.format('(%dx%d) %d box, width:%d ~ %d' \n",
    "                       %(img_h, img_w, len(boxes), box_min_width, box_max_width))\n",
    "    plt.title(title)\n",
    "    for box, cls in zip(boxes, classes):\n",
    "        x1, y1, x2, y2 = box        \n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        color = edgecolors[int(cls)]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 70:\n",
    "            score_txt = class_names[int(cls)]\n",
    "            ax.text(x1, y1, score_txt, bbox={\"facecolor\": [1,1,0], \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "        \n",
    "    plt.show()\n",
    "    return ax\n",
    "\n",
    "def display_data(X, BBOX, stride=1):\n",
    "    for i in range(len(X)):\n",
    "        if i%stride==0:\n",
    "            img_arr = X[i]\n",
    "            sample_box = BBOX[i]\n",
    "            label = sample_box[:, 0]\n",
    "            bbox = sample_box[:, 1:]\n",
    "\n",
    "            h, w, c = img_arr.shape\n",
    "            scale = np.array((w, h, w, h))\n",
    "            scale = np.reshape(scale, (1, 4))\n",
    "            bbox_norm = bbox.astype(np.float) * scale.astype(np.float)\n",
    "            #print('bbox_norm', bbox, bbox_norm)\n",
    "            print(i, np.unique(label))\n",
    "            ax = visualize_detections_simple(img_arr,bbox_norm,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_object(X, BBOX, scope=0.5):\n",
    "    \n",
    "    crop_xs = []\n",
    "    crop_bboxs = []\n",
    "    for i in range(len(X)):\n",
    "        x = X[i]\n",
    "        img_h, img_w, img_c = x.shape\n",
    "        bbox = BBOX[i]\n",
    "        #print('len', len(x), len(bbox), x.shape, bbox.shape)\n",
    "        cls = bbox[:, 0]\n",
    "        x0 = bbox[:, 1]\n",
    "        y0 = bbox[:, 2]\n",
    "        x1 = bbox[:, 3]\n",
    "        y1 = bbox[:, 4]\n",
    "        box_h = y1 - y0\n",
    "        box_w = x1 - x0\n",
    "        box_y_min = np.min(y0)\n",
    "        box_x_min = np.min(x0)        \n",
    "        box_y_max = np.max(y1)\n",
    "        box_x_max = np.max(x1)        \n",
    "        if box_y_max - box_y_min < scope and box_x_max - box_x_min < scope:\n",
    "            \n",
    "            cx = np.mean((box_x_min + box_x_max)/2)\n",
    "            \n",
    "            if cx < 0.5:\n",
    "                tx0 = np.maximum(0, cx - scope/2)\n",
    "                tx1 = tx0 + scope\n",
    "            else:\n",
    "                tx1 = np.minimum(1.0, cx + scope/2)\n",
    "                tx0 = tx1 - scope\n",
    "                        \n",
    "            tbox = np.stack((cls, (x0 - tx0)/scope, y0, (x1 - tx0)/scope, y1), axis=1)            \n",
    "            \n",
    "            img_x0 = int(tx0 * img_w)\n",
    "            img_x1 = img_x0 + int(img_w*scope)\n",
    "            timg = x[:, img_x0:img_x1]\n",
    "            \n",
    "            img = Image.fromarray(timg)\n",
    "            img_resized = img.resize((padded_image_shape[1]//2, padded_image_shape[0]))\n",
    "            arr_resized = np.array(img_resized)            \n",
    "            \n",
    "            crop_xs.append(arr_resized)\n",
    "            crop_bboxs.append(tbox)\n",
    "    return crop_xs, crop_bboxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def attach_tiled_data(X, BBOX, row=2, col=2):\n",
    "    m = len(X)\n",
    "    attach_m = int(np.ceil(m/(row*col)))\n",
    "    attach_xs = []\n",
    "    attach_bboxs = []\n",
    "    img_h, img_w, img_c = X[0].shape\n",
    "    for i in range(attach_m):\n",
    "        bg_color = np.median(X[i])\n",
    "        attach_xs.append(bg_color + np.zeros((img_h*row, img_w*col, img_c)))    \n",
    "        attach_bboxs.append([])\n",
    "    \n",
    "    m_rand = np.arange(m)\n",
    "    np.random.shuffle(m_rand)\n",
    "    for i in range(len(m_rand)):\n",
    "        j = i#m_rand[i]\n",
    "        x = X[j]\n",
    "        bbox = BBOX[j]        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        ti = i//(row*col)\n",
    "        ty = i%(row*col)//col\n",
    "        tx = i%(row*col)%col\n",
    "        dst_y0 = ty * img_h\n",
    "        dst_y1 = dst_y0 + img_h\n",
    "        dst_x0 = tx * img_w\n",
    "        dst_x1 = dst_x0 + img_w\n",
    "               \n",
    "        attach_xs[ti][dst_y0:dst_y1, dst_x0:dst_x1] = x                    \n",
    "        cls, x0, y0, x1, y1 = np.split(bbox, 5, -1)\n",
    "        \n",
    "        x_scale = 1.0 / col\n",
    "        y_scale = 1.0 / row\n",
    "        x0 = x0 * x_scale + tx * x_scale\n",
    "        y0 = y0 * y_scale + ty * y_scale\n",
    "        x1 = x1 * x_scale + tx * x_scale\n",
    "        y1 = y1 * y_scale + ty * y_scale\n",
    "        bbox = np.concatenate((cls, x0, y0, x1, y1), axis=1)        \n",
    "        attach_bboxs[ti].extend(bbox)\n",
    "            \n",
    "    for i in range(len(attach_bboxs)):\n",
    "        attach_bboxs[i] = np.stack(attach_bboxs[i], 0)\n",
    "        \n",
    "    return attach_xs, attach_bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_convert_cxy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate(( (y0+y1)/2, (x0+x1)/2 ), axis=1)\n",
    "\n",
    "def box_swap_xy(box):\n",
    "    y0, x0, y1, x1 = np.split(box, 4, axis=-1)    \n",
    "    return np.concatenate((x0, y0, x1, y1), axis=1)\n",
    "\n",
    "def box_convert_to_xywh(boxes):\n",
    "    return np.concatenate(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]], axis=-1,)\n",
    "\n",
    "def box_convert_to_corners(boxes):    \n",
    "    return np.concatenate(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0], axis=-1,)\n",
    "\n",
    "def angle_to_radian(angle):\n",
    "    return angle * np.pi/180\n",
    "\n",
    "def rotate_images(X, angle):\n",
    "    rotate_X = []\n",
    "    for i in range(len(X)):        \n",
    "        x = X[i]\n",
    "        img_h = x.shape[0]\n",
    "        img_w = x.shape[1]\n",
    "        img = Image.fromarray(x)                \n",
    "        img_rotated = img.rotate(angle)\n",
    "        rotate_X.append(np.array(img_rotated))\n",
    "\n",
    "    return rotate_X    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_kor, class_names_kor.index('사람')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_rotate_data(X, BBOX, angle):\n",
    "    rotate_xs = []\n",
    "    rotate_bboxs = []\n",
    "    m = len(X)\n",
    "    for i in range(m):        \n",
    "        x = X[i]\n",
    "        \n",
    "        bbox = BBOX[i]\n",
    "        cls = bbox[:, 0]\n",
    "        x0 = bbox[:, 1]\n",
    "        y0 = bbox[:, 2]\n",
    "        x1 = bbox[:, 3]\n",
    "        y1 = bbox[:, 4]\n",
    "        \n",
    "        if class_names_kor.index('사람') in cls.astype(np.int):\n",
    "            print('skip person contain image', i, cls)\n",
    "            continue\n",
    "        \n",
    "        if class_names_kor.index('큰부리까마귀') + 0.1 >= np.max(cls):\n",
    "            print('skip only 큰부리까마귀', i, cls)\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        box = bbox[:, 1:]\n",
    "        box_xywh = box_convert_to_xywh(box)\n",
    "        box_xy = box_xywh[:, :2] \n",
    "        box_wh = box_xywh[:, 2:] \n",
    "        box_uv = (np.reshape(box_xy, [-1, 2]) - 0.5) * 2\n",
    "        \n",
    "        img_h, img_w, img_c = x.shape\n",
    "        img = Image.fromarray(x)        \n",
    "        scale_mat = np.array([1, 0, 0, 1.0*img_h/img_w]).reshape((2,2))\n",
    "        scale_mat_rev = np.array([1, 0, 0, 1.0*img_w/img_h]).reshape((2,2))        \n",
    "        \n",
    "        radian = angle_to_radian(angle)        \n",
    "        rotate_mat = np.array([np.cos(radian), -np.sin(radian), np.sin(radian), np.cos(radian)])        \n",
    "        rotate_mat = np.reshape(rotate_mat, (2, 2))\n",
    "        box_uv_trans = np.matmul(box_uv, scale_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, rotate_mat)\n",
    "        box_uv_trans = np.matmul(box_uv_trans, scale_mat_rev)\n",
    "        box_trans = (box_uv_trans + 1)/2\n",
    "        box_trans_xy = np.reshape(box_trans, [-1, 2])\n",
    "        box_trans_xywh = np.concatenate((box_trans_xy, box_wh), axis=1)\n",
    "        box_trans = box_convert_to_corners(box_trans_xywh)\n",
    "   \n",
    "        #if np.min(box_trans)<0 or np.max(box_trans)>1: continue\n",
    "        \n",
    "        bbox_trans = np.concatenate((np.expand_dims(cls, 1), box_trans), axis=1)\n",
    "   \n",
    "        img_rotated = img.rotate(angle)\n",
    "        #plt.imshow(img_rotated)\n",
    "        rotate_xs.append(np.array(img_rotated))\n",
    "        rotate_bboxs.append(bbox_trans)\n",
    "\n",
    "    return rotate_xs, rotate_bboxs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_flip_horizontal(X, Y):\n",
    "    X_ = []\n",
    "    Y_ = []\n",
    "    for i in range(len(Y)):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        cls, x1, y1, x2, y2 = np.split(y, 5, -1)\n",
    "        print(i, cls, x1, y1, x2, y2)\n",
    "        X_.append(x[:,::-1])\n",
    "        y_flip_h = np.concatenate([cls, 1-x2, y1, 1-x1, y2], -1)\n",
    "        Y_.append(y_flip_h)\n",
    "    \n",
    "    return X_, Y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_list), len(input_list_pig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = len(input_list)\n",
    "print('m', m)\n",
    "input_list_test_valid = input_list_pig[1::2]\n",
    "bbox_list_test_valid = bbox_list_pig[1::2]\n",
    "\n",
    "input_list_test = input_list_test_valid[1::2]\n",
    "bbox_list_test = bbox_list_test_valid[1::2]\n",
    "\n",
    "input_list_train = input_list_pig[::2] + input_list_test_valid[::2]# + input_list\n",
    "bbox_list_train = bbox_list_pig[::2] + bbox_list_test_valid[::2]# + bbox_list \n",
    "\n",
    "print('bbox_list_train', len(bbox_list), len(bbox_list_train), len(bbox_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rotate, y_rotate = gen_rotate_data(input_list_train, bbox_list_train, 2.0)\n",
    "x_rotate1, y_rotate1 = gen_rotate_data(input_list_train, bbox_list_train, -2.0)\n",
    "x_rotate2, y_rotate2 = gen_rotate_data(input_list_train, bbox_list_train, 3.0)\n",
    "x_rotate3, y_rotate3 = gen_rotate_data(input_list_train, bbox_list_train, -3.0)\n",
    "\n",
    "#display_data(x_rotate, y_rotate, stride=1)\n",
    "input_list_train.extend(x_rotate)\n",
    "bbox_list_train.extend(y_rotate)\n",
    "input_list_train.extend(x_rotate1)\n",
    "bbox_list_train.extend(y_rotate1)\n",
    "\n",
    "input_list_train.extend(x_rotate2)\n",
    "bbox_list_train.extend(y_rotate2)\n",
    "input_list_train.extend(x_rotate3)\n",
    "bbox_list_train.extend(y_rotate3)\n",
    "\n",
    "print(len(x_rotate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_flip, Y_flip = augment_flip_horizontal(input_list_train, bbox_list_train)\n",
    "input_list_train.extend(X_flip)\n",
    "bbox_list_train.extend(Y_flip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('len', len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_data(input_list_train, bbox_list_train, stride=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps order the of x and y coordinates of the boxes.\n",
    "    Arguments:\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "    Returns:\n",
    "      swapped boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height.\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2.0, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "    Arguments:\n",
    "      boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      converted boxes with shape same as that of boxes.\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2.0, boxes[..., :2] + boxes[..., 2:] / 2.0],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n",
    "\"\"\"\n",
    "\n",
    "def compute_iou(boxes1, boxes2):#compute_iou(anchor_boxes, gt_boxes)\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "    Arguments:\n",
    "      boxes1: A tensor with shape `(N, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes\n",
    "        where each box is of the format `[x, y, width, height]`.\n",
    "    Returns:\n",
    "      pairwise IOU matrix with shape `(N, M)`, where the value at ith row\n",
    "        jth column holds the IOU between ith box and jth box from\n",
    "        boxes1 and boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "    lu = tf.maximum(boxes1_corners[:, None, :2], boxes2_corners[:, :2])\n",
    "    rd = tf.minimum(boxes1_corners[:, None, 2:], boxes2_corners[:, 2:])\n",
    "    intersection = tf.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[:, :, 0] * intersection[:, :, 1]\n",
    "    boxes1_area = boxes1[:, 2] * boxes1[:, 3]\n",
    "    boxes2_area = boxes2[:, 2] * boxes2[:, 3]\n",
    "    union_area = tf.maximum(\n",
    "        boxes1_area[:, None] + boxes2_area - intersection_area, 1e-8\n",
    "    )\n",
    "    return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_end - level_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Implementing Anchor generator\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding\n",
    "box for an object. It does this by regressing the offset between the location\n",
    "of the object's center and the center of an anchor box, and then uses the width\n",
    "and height of the anchor box to predict a relative scale of the object. In the\n",
    "case of RetinaNet, each location on a given feature map has nine anchor boxes\n",
    "(at three scales and three ratios).\n",
    "\"\"\"\n",
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes.\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Where each anchor each box is of the\n",
    "    format `[x, y, width, height]`.\n",
    "    Attributes:\n",
    "      aspect_ratios: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map\n",
    "      scales: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "      num_anchors: The number of anchor boxes at each location on feature map\n",
    "      areas: A list of float values representing the areas of the anchor\n",
    "        boxes for each feature map in the feature pyramid.\n",
    "      strides: A list of float value representing the strides for each feature\n",
    "        map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.level_start = level_start\n",
    "        self.level_end = level_end\n",
    "        \n",
    "        if anchor_k==9:\n",
    "            self.aspect_ratios = [0.5, 1.0, 2.0]        \n",
    "            self.scales = [2 ** x for x in [0, 1 / 3, 2 / 3]]\n",
    "        else:\n",
    "            self.aspect_ratios = [1.0]        \n",
    "            self.scales = [2 ** x for x in [0]]\n",
    "                \n",
    "        self._num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "        self._strides = [2 ** i for i in range(self.level_start, self.level_end)]\n",
    "        self._areas = [x ** 2 for x in [32.0, 64.0, 128.0, 196.0, 256.0]]                        \n",
    "        self._areas = self._areas[:level_end - level_start]\n",
    "        \n",
    "        self._anchor_dims = self._compute_dims()\n",
    "\n",
    "    def _compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\n",
    "        \"\"\"\n",
    "        anchor_dims_all = []\n",
    "        for area in self._areas:\n",
    "            anchor_dims = []\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)\n",
    "                anchor_width = area / anchor_height\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], axis=-1), [1, 1, 2]\n",
    "                )\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "        return anchor_dims_all\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "        Arguments:\n",
    "          feature_height: An integer representing the height of the feature map.\n",
    "          feature_width: An integer representing the width of the feature map.\n",
    "          level: An integer representing the level of the feature map in the\n",
    "            feature pyramid.\n",
    "        Returns:\n",
    "          anchor boxes with the shape\n",
    "          `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - self.level_start]\n",
    "        centers = tf.expand_dims(centers, axis=-2)\n",
    "        centers = tf.tile(centers, [1, 1, self._num_anchors, 1])\n",
    "        dims = tf.tile(\n",
    "            self._anchor_dims[level - self.level_start], [feature_height, feature_width, 1, 1]\n",
    "        )\n",
    "        anchors = tf.concat([centers, dims], axis=-1)\n",
    "        return tf.reshape(\n",
    "            anchors, [feature_height * feature_width * self._num_anchors, 4]\n",
    "        )\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return tf.concat(anchors, axis=0)\n",
    "    \n",
    "    def get_anchors_check(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "        Arguments:\n",
    "          image_height: Height of the input image.\n",
    "          image_width: Width of the input image.\n",
    "        Returns:\n",
    "          anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / 2 ** i),\n",
    "                tf.math.ceil(image_width / 2 ** i),\n",
    "                i,\n",
    "            )\n",
    "            for i in range(self.level_start, self.level_end)\n",
    "        ]\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_check = AnchorBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anchors = anchor_check.get_anchors_check(128,128)\n",
    "for anchor in anchors:\n",
    "    print(anchor.shape, anchor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image_shape, 128*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Preprocessing data\n",
    "Preprocessing the images involves two steps:\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "Along with the images, bounding boxes are rescaled and flipped if required.\n",
    "\"\"\"\n",
    "\n",
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes,\n",
    "        having normalized coordinates.\n",
    "    Returns:\n",
    "      Randomly flipped image and boxes\n",
    "    \"\"\"\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack([1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]], axis=-1)\n",
    "   \n",
    "    return image, boxes\n",
    "\n",
    "def random_flip_vertical(image, boxes):\n",
    "    is_flipped = tf.zeros_like(boxes[:, 0])\n",
    "    if tf.random.uniform(()) > 10.5:\n",
    "        image = tf.image.flip_up_down(image)        \n",
    "        boxes = tf.stack([boxes[:, 0], 1 - boxes[:, 3], boxes[:, 2], 1 - boxes[:, 1]], axis=-1)\n",
    "        is_flipped = tf.ones_like(is_flipped)\n",
    "        \n",
    "    return image, boxes, is_flipped\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, mask_obj=None, min_side=padded_image_shape[0], max_side=11333.0, jitter=[128*4, 128*4+1], stride=128.0\n",
    "):\n",
    "   \n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    \n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    \n",
    "    image_shape = ratio * image_shape\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    \n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1]) \n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    if mask_obj!=None:\n",
    "        return image, image_shape, ratio, mask_obj\n",
    "    return image, image_shape, ratio\n",
    "\n",
    "def resize_and_pad_image_bbox(#resize\n",
    "    image, bbox, mask_obj=None, min_side=padded_image_shape[0], \n",
    "    max_side=1024.0*4, jitter=[128*3+32, 128*4-32], stride=128.0\n",
    "):\n",
    "    #image, min_side=800.0, max_side=1333.0, jitter=[640, 1024], stride=128.0\n",
    "    \"\"\"Resizes and pads image while preserving aspect ratio.\n",
    "    1. Resizes images so that the shorter side is equal to `min_side`\n",
    "    2. If the longer side is greater than `max_side`, then resize the image\n",
    "      with longer side equal to `max_side`\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`\n",
    "    Arguments:\n",
    "      image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "      min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "      max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "      jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be\n",
    "        resized to a random value in this range.\n",
    "      stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "    Returns:\n",
    "      image: Resized and padded image.\n",
    "      image_shape: Shape of the image before padding.\n",
    "      ratio: The scaling factor used to resize the image\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "    if jitter is not None:\n",
    "        min_side = tf.random.uniform((), jitter[0], jitter[1], dtype=tf.float32)\n",
    "    ratio = min_side / tf.reduce_min(image_shape)\n",
    "    if ratio * tf.reduce_max(image_shape) > max_side:\n",
    "        ratio = max_side / tf.reduce_max(image_shape)\n",
    "    image_shape = ratio * image_shape\n",
    "    ratio_jitter = tf.random.uniform(tf.shape(image_shape), -32, 32, dtype=tf.float32)\n",
    "    image_shape += ratio_jitter      \n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.resize(mask_obj, tf.cast(image_shape, dtype=tf.int32))\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride, dtype=tf.int32\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(image, 0, 0, padded_image_shape[0], padded_image_shape[1])\n",
    "    if mask_obj!=None:\n",
    "        mask_obj = tf.image.pad_to_bounding_box(mask_obj, 0, 0, padded_image_shape[0], padded_image_shape[1])        \n",
    "    padded_image_shape = tf.cast(padded_image_shape, tf.float32)              \n",
    "    pad_ratio = tf.cast(image_shape, tf.float32) / padded_image_shape\n",
    "    bbox_padded = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * pad_ratio[1],\n",
    "            bbox[:, 1] * pad_ratio[0],\n",
    "            bbox[:, 2] * pad_ratio[1],\n",
    "            bbox[:, 3] * pad_ratio[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    if mask_obj!=None:\n",
    "        return image, padded_image_shape, ratio, bbox_padded, mask_obj    \n",
    "    return image, padded_image_shape, ratio, bbox_padded\n",
    "\n",
    "\n",
    "def preprocess_data(image, cls_bbox):\n",
    "    \"\"\"Applies preprocessing step to a single sample\n",
    "    Arguments:\n",
    "      sample: A dict representing a single training sample.\n",
    "    Returns:\n",
    "      image: Resized and padded image with random horizontal flipping applied.\n",
    "      bbox: Bounding boxes with the shape `(num_objects, 4)` where each box is\n",
    "        of the format `[x, y, width, height]`.\n",
    "      class_id: An tensor representing the class id of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "     \n",
    "    bbox = cls_bbox[:, 1:]\n",
    "    class_id = tf.cast(cls_bbox[:, 0], dtype=tf.int32)    \n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)\n",
    "    image, bbox, is_flipped = random_flip_vertical(image, bbox)    \n",
    "    is_flipped = tf.cast(is_flipped, tf.int32)\n",
    "    image, image_shape, _, bbox = resize_and_pad_image_bbox(image, bbox)\n",
    "        \n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    cls_flip = tf.stack((class_id, is_flipped), -1)\n",
    "    return image, bbox, cls_flip\n",
    "\n",
    "\n",
    "def preprocess_test_data(image, cls_bbox):         \n",
    "    bbox = cls_bbox[:, 1:]\n",
    "    class_id = tf.cast(cls_bbox[:, 0], dtype=tf.int32)        \n",
    "    \n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "    is_flipped = tf.zeros_like(class_id)\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)    \n",
    "    cls_flip = tf.stack((class_id, is_flipped), -1)\n",
    "    return image, bbox, cls_flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_color_augment(x):\n",
    "    if tf.random.uniform(()) < -0.5:\n",
    "        x_max = tf.reduce_max(x, [1, 2], True)\n",
    "        x = x_max - x    \n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_hue(x, 0.08)\n",
    "        x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = tf.image.random_brightness(x, 0.05)\n",
    "        x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        gray = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.concat((gray, gray, gray), -1)        \n",
    "    if tf.random.uniform(()) < -0.2:\n",
    "        noise = tf.random.normal(tf.shape(x), stddev=tf.pow(tf.reduce_mean(x), 0.3))\n",
    "        x += noise\n",
    "    if tf.random.uniform(()) < 0.2:\n",
    "        x = gaussian_filter2d(x, filter_shape=tuple(np.random.randint(1, 10, (2))), sigma=10)\n",
    "        #x = gaussian_filter2d(x, filter_shape=np.random.randint(3, 10, (2)), sigma=10)\n",
    "    if tf.random.uniform(()) < 0.2:        \n",
    "        x = sharpness(x, factor=10)\n",
    "        \n",
    "    #x = tf.image.random_jpeg_quality(x, 0, 1.0)\n",
    "    #x = tf.clip_by_value(x, 0, 1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Encoding labels\n",
    "The raw labels, consisting of bounding boxes and class ids need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n",
    "\"\"\"\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Transforms the raw labels into targets for training.\n",
    "    This class has operations to generate targets for a batch of samples which\n",
    "    is made up of the input images, bounding boxes for the objects present and\n",
    "    their class ids.\n",
    "    Attributes:\n",
    "      anchor_box: Anchor box generator to encode the bounding boxes.\n",
    "      box_variance: The scaling factors used to scale the bounding box targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._anchor_box = AnchorBox()\n",
    "        self._box_variance = tf.convert_to_tensor(\n",
    "            [0.1, 0.1, 0.2, 0.2], dtype=tf.float32\n",
    "        )    \n",
    "    \n",
    "    def _compute_box_target(self, anchor_boxes, matched_gt_boxes):\n",
    "        \"\"\"Transforms the ground truth boxes into targets for training\"\"\"\n",
    "        box_target = tf.concat(\n",
    "            [\n",
    "                (matched_gt_boxes[:, :2] - anchor_boxes[:, :2]) / anchor_boxes[:, 2:],\n",
    "                tf.math.log(matched_gt_boxes[:, 2:] / anchor_boxes[:, 2:]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        )\n",
    "        box_target = box_target / self._box_variance\n",
    "        return box_target\n",
    "\n",
    "    def _match_anchor_boxes(\n",
    "        self, anchor_boxes, gt_boxes, match_iou=0.5, ignore_iou=0.25\n",
    "    ):\n",
    "        iou_matrix = compute_iou(anchor_boxes, gt_boxes)\n",
    "        max_iou = tf.reduce_max(iou_matrix, axis=1)#from anchor to object-box        \n",
    "        matched_gt_idx = tf.argmax(iou_matrix, axis=1)    \n",
    "        positive_mask = tf.greater_equal(max_iou, match_iou)# not only this, but also need max iou cell\n",
    "        \n",
    "        positive_proposal_mask = tf.greater_equal(iou_matrix, match_iou)\n",
    "        positive_mask = tf.reduce_any(positive_proposal_mask, axis=1)\n",
    "        \n",
    "        negative_mask = tf.less(max_iou, ignore_iou)\n",
    "        \n",
    "        max_iou_anchor = tf.reduce_max(iou_matrix, axis=0, keepdims=True) \n",
    "        max_iou_anchor_mask = tf.greater_equal(iou_matrix, max_iou_anchor)\n",
    "        positive_max_mask = tf.reduce_any(max_iou_anchor_mask, axis=1)\n",
    "        positive_mask = tf.logical_or(positive_mask, positive_max_mask)#new      \n",
    "        \n",
    "        negative_mask = tf.logical_and(negative_mask, tf.logical_not(positive_mask))\n",
    "        ignore_mask = tf.logical_not(tf.logical_or(positive_mask, negative_mask))        \n",
    "        \n",
    "        return (\n",
    "            matched_gt_idx,            \n",
    "            tf.cast(positive_mask, dtype=tf.float32),\n",
    "            tf.cast(positive_max_mask, dtype=tf.float32),            \n",
    "            tf.cast(ignore_mask, dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "    def _encode_sample(self, image_shape, gt_boxes, cls_ids):\n",
    "        \"\"\"Creates box and classification targets for a single sample\"\"\"\n",
    "        \n",
    "        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "        cls_ids = tf.cast(cls_ids, dtype=tf.float32)\n",
    "        matched_gt_idx, positive_mask, positive_max_mask, ignore_mask = self._match_anchor_boxes(\n",
    "            anchor_boxes, gt_boxes\n",
    "        )\n",
    "        matched_gt_boxes = tf.gather(gt_boxes, matched_gt_idx)\n",
    "        matched_gt_boxes_size = tf.reduce_prod(matched_gt_boxes[:, 2:], 1)\n",
    "        matched_gt_boxes_size = tf.sqrt(matched_gt_boxes_size)        \n",
    "        \n",
    "        box_target = self._compute_box_target(anchor_boxes, matched_gt_boxes)    \n",
    "        matched_gt_cls_ids = tf.gather(cls_ids, matched_gt_idx)\n",
    "        cls_target = tf.where(\n",
    "            tf.not_equal(positive_mask, 1.0), 0.0, matched_gt_cls_ids\n",
    "        )\n",
    "        cls_target = tf.where(tf.equal(ignore_mask, 1.0), -1.0, cls_target)       \n",
    "        cls_target = tf.expand_dims(cls_target, axis=-1)        \n",
    "        positive_max_mask= tf.expand_dims(positive_max_mask, -1)\n",
    "        label = tf.concat([box_target, cls_target, positive_max_mask], axis=-1)        \n",
    "        return label\n",
    "    \n",
    "    def encode_batch(self, batch_images, gt_boxes, cls_flip):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        #is_flipped 0 or 1\n",
    "        images_shape = tf.shape(batch_images)\n",
    "        batch_size = images_shape[0]\n",
    "        cls_ids, is_flipped = tf.split(cls_flip, 2, -1)\n",
    "        cls_ids = tf.squeeze(cls_ids, -1)\n",
    "        is_flipped = tf.squeeze(is_flipped, -1)\n",
    "        is_flipped = tf.cast(is_flipped, tf.float32)\n",
    "        \n",
    "        labels = tf.TensorArray(dtype=tf.float32, size=batch_size, dynamic_size=True)\n",
    "        for i in range(batch_size):\n",
    "            label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n",
    "            labels = labels.write(i, label)\n",
    "        \n",
    "        batch_images = tf.cast(batch_images, tf.float32)\n",
    "        label = labels.stack()\n",
    "        is_flipped_anchor = tf.zeros_like(label[:, :, :1]) + tf.reduce_max(is_flipped)\n",
    "        label = tf.concat((label, is_flipped_anchor), -1)\n",
    "        return batch_images, label      \n",
    "    \n",
    "    def encode_batch_train(self, batch_images, gt_boxes, cls):\n",
    "        \"\"\"Creates box and classification targets for a batch\"\"\"\n",
    "        \n",
    "        batch_images = image_color_augment(batch_images)#new        \n",
    "        return self.encode_batch(batch_images, gt_boxes, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "from tensorflow import Tensor\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BifeaturePyramidNet(c345):\n",
    "    regulizer  = tf.keras.regularizers.L2(l1)\n",
    "    filters = 256    \n",
    "    a2 = c345[0]\n",
    "    a3 = c345[1]\n",
    "    a4 = c345[2]\n",
    "    a5 = c345[3]\n",
    "    a6 = Conv2D(filters, 3, 2, \"same\", activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    a7 = Conv2D(filters, 3, 2, \"same\", activation=activation, kernel_regularizer=regulizer)(a6)\n",
    "    #a8 = Conv2D(filters, 3, 2, \"same\", activation=activation, kernel_regularizer=regulizer)(a7)\n",
    "    \n",
    "    #b3 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #b4 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a2_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a2)\n",
    "    #a3_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_0 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    #a3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    #a4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    \n",
    "    a33 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a3)\n",
    "    a44 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a4)\n",
    "    a55 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a5)\n",
    "    a66 = Conv2D(filters*2, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(a6)\n",
    "    a3_0, a3_1 = tf.split(a33, 2, -1)\n",
    "    a4_0, a4_1 = tf.split(a44, 2, -1)\n",
    "    a5_0, a5_1 = tf.split(a55, 2, -1)\n",
    "    a6_0, a6_1 = tf.split(a66, 2, -1)\n",
    "    \n",
    "    a7_up = keras.layers.UpSampling2D(2)(a7)    \n",
    "    b6 = keras.layers.Add()([a6_0, a7_up])  \n",
    "    \n",
    "    a6_up = keras.layers.UpSampling2D(2)(a6_0)    \n",
    "    b5 = keras.layers.Add()([a5_0, a6_up])  \n",
    "    \n",
    "    a5_up = keras.layers.UpSampling2D(2)(a5_0)    \n",
    "    b4 = keras.layers.Add()([a4_0, a5_up])  \n",
    "    \n",
    "    b4_up = keras.layers.UpSampling2D(2)(b4)\n",
    "    b3 = keras.layers.Add()([a3_0, b4_up])  \n",
    "    \n",
    "    b3_up = keras.layers.UpSampling2D(2)(b3)\n",
    "    b2 = keras.layers.Add()([a2_0, b3_up])\n",
    "    \n",
    "    b2_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b2)\n",
    "    b3_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b3)    \n",
    "    c3 = keras.layers.Add()([a3_1, b3_1, b2_down])\n",
    "    \n",
    "    c3_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c3)\n",
    "    b4_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b4)    \n",
    "    c4 = keras.layers.Add()([a4_1, b4_1, c3_down])\n",
    "    \n",
    "    c4_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c4)\n",
    "    b5_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b5)    \n",
    "    c5 = keras.layers.Add()([a5_1, b5_1, c4_down])    \n",
    "    \n",
    "    c5_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c5) \n",
    "    b6_1 = Conv2D(filters, 1, 1, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(b6)    \n",
    "    c6 = keras.layers.Add()([a6, b6_1, c5_down]) \n",
    "    \n",
    "    c6_down = Conv2D(filters, 3, 2, \"same\", groups=1, activation=activation, kernel_regularizer=regulizer)(c6)\n",
    "    c7 = keras.layers.Add()([a7, c6_down]) \n",
    "    \n",
    "    return c3, c4, c5, c6, c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/image/__init__.py\n",
    "from tensorflow_addons.image.color_ops import sharpness\n",
    "from tensorflow_addons.image.filters import gaussian_filter2d\n",
    "from tensorflow_addons.image.dense_image_warp import dense_image_warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))  # 18   \n",
    "outputs = Conv2D(10, 3)(inputs)# 18 * 10 + 10 = 190\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backbone = keras.applications.EfficientNetB3(include_top=False, input_shape=[128, 128, 3])\n",
    "#backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs = Input(shape=(3, 3, 2))      # 9 + 9\n",
    "outputs = Conv2D(10, 3, groups=2)(inputs) # 9*5 + 5 + 9*5 + 5\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.EfficientNetB2(include_top=False, input_shape=[None, None, 3])\n",
    "    c2_output, c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in ['block2c_add', 'block3c_add', 'block5d_add', 'block6e_add']]\n",
    "    #c4_output = (c4_output + c4a_output[:, :, :, :80])/2\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c2_output, c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()\n",
    "#D0 for layer_name in [\"block2b_add\", \"block3b_add\", \"block5c_add\", \"block6d_add\"]]\n",
    "#D2 [\"block2c_add\", \"block3c_add\", \"block5d_add\",'block6e_add']\n",
    "#D3 ['block2c_add', 'block3c_add', 'block5e_add', 'top_activation']\n",
    "#D4 ['block2d_add', 'block3d_add', 'block5f_add', 'top_activation']\n",
    "#D7 for layer_name in [\"block2f_add\", \"block3g_add\", \"block5j_add\", \"block6d_add\"]]\n",
    "#input                           (None, 64, 64, 3)   \n",
    "#block2b_add (Add)               (None, 16, 16, 24) \n",
    "#block3b_add (Add)               (None, 8, 8, 40)    \n",
    "#block4c_add (Add)               (None, 4, 4, 80)\n",
    "#block5c_add (Add)               (None, 4, 4, 112) \n",
    "#block6d_add (Add)               (None, 2, 2, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone.trainable = True #finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_backbone():\n",
    "    backbone = keras.applications.MobileNetV2(include_top=False, input_shape=[None, None, 3])\n",
    "    c3_output, c4_output, c5_output = [\n",
    "        backbone.get_layer(layer_name).output\n",
    "        for layer_name in [\"block_6_expand_relu\", \"block_13_expand_relu\", \"out_relu\"]]\n",
    "    return keras.Model(\n",
    "        inputs=[backbone.inputs], outputs=[c3_output, c4_output, c5_output]\n",
    "    )\n",
    "backbone = get_backbone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_start, level_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetinaNet(num_classes, anchor_k, is_train=False):\n",
    "    prior_probability = tf.constant_initializer(-np.log((1 - 0.01) / 0.01))\n",
    "    inputs = Input(shape=(None, None, 3))        \n",
    "    \n",
    "    nets_3 = backbone(inputs, training=is_train)    \n",
    "    #nets_3 = create_resnet_backbone(inputs / 255)            \n",
    "    features = BifeaturePyramidNet(nets_3)    \n",
    "    \n",
    "    cls_outputs = []\n",
    "    box_outputs = []\n",
    "   \n",
    "    kernel_init = tf.initializers.he_normal()\n",
    "    regulizer = tf.keras.regularizers.L2(l1)\n",
    "    #conv_c0 = keras.layers.Conv2D(256, 3, padding=\"same\", activation=activation, kernel_initializer=kernel_init, kernel_regularizer=regulizer)    \n",
    "    \n",
    "    drop = keras.layers.Dropout(0.1)\n",
    "    \n",
    "    cbox_outputs = []\n",
    "    N = tf.shape(inputs)[0]# TFLite cannot convert tf.shape() function\n",
    "    #N = 1\n",
    "        \n",
    "    for i in range(0, len(features)):                    \n",
    "        conv_0 = keras.layers.Conv2D(anchor_k * (4+num_classes), 3, 1, padding=\"same\", kernel_initializer=kernel_init, bias_initializer=prior_probability, name='head_conv_'+str(i))   \n",
    "                \n",
    "        cls_out = conv_0(drop(features[i]))\n",
    "        cbox_out = tf.reshape(cls_out, [N, -1, 4+num_classes]) \n",
    "        cbox_outputs.append(cbox_out)\n",
    "    \n",
    "    outputs = tf.concat(cbox_outputs, axis=1)  \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)#dual    \n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _decode_box_predictions(anchor_boxes, box_predictions):\n",
    "    _box_variance = tf.convert_to_tensor([0.1, 0.1, 0.2, 0.2], dtype=tf.float32)\n",
    "    boxes = box_predictions * _box_variance\n",
    "    boxes = tf.concat(\n",
    "        [\n",
    "            boxes[:, :, :2] * anchor_boxes[:, :, 2:] + anchor_boxes[:, :, :2],\n",
    "            tf.math.exp(boxes[:, :, 2:]) * anchor_boxes[:, :, 2:],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    boxes_transformed = convert_to_corners(boxes)\n",
    "    return boxes_transformed\n",
    "\n",
    "def decodePredictions(images, predictions, \n",
    "                      num_classes=num_classes,\n",
    "                      confidence_threshold=0.5,\n",
    "                      nms_iou_threshold=0.2,\n",
    "                      max_detections_per_class=1000,\n",
    "                      max_detections=1500,\n",
    "                      box_variance=[0.1, 0.1, 0.2, 0.2]):\n",
    "    \n",
    "    _anchor_box = AnchorBox()\n",
    "        \n",
    "    image_shape = tf.cast(tf.shape(images), dtype=tf.float32)\n",
    "    #image_h = padded_image_shape[0]\n",
    "    #image_w = padded_image_shape[1]\n",
    "    anchor_boxes = _anchor_box.get_anchors(image_shape[1], image_shape[2])\n",
    "    #anchor_boxes = _anchor_box.get_anchors(image_h, image_w)\n",
    "    box_predictions = predictions[:, :, :4]\n",
    "    objectness = tf.nn.sigmoid(predictions[:, :, 4:5])\n",
    "    cls_score = predictions[:, :, 5:5+num_classes_real]\n",
    "    cls_prob = tf.nn.softmax(cls_score)\n",
    "    cls_prob_max = tf.reduce_max(cls_prob, -1)\n",
    "   \n",
    "    cls = tf.argmax(cls_score, -1)\n",
    "    cls = tf.cast(cls, tf.float32)\n",
    "    \n",
    "    boxes = _decode_box_predictions(anchor_boxes[None, ...], box_predictions)\n",
    "    boxes_2d = tf.reshape(boxes, [-1, 4])    \n",
    "    scores = tf.reshape(objectness, [-1, 1])#new\n",
    "    #scores = tf.sqrt(scores * tf.reshape(cls_prob_max, [-1, 1]))#new\n",
    "    cls = tf.reshape(cls, [-1, 1])\n",
    "    ccbox = tf.concat((cls, scores, boxes_2d), -1)\n",
    "    \n",
    "    selected_indices, selected_scores = tf.image.non_max_suppression_with_scores(    \n",
    "        ccbox[:, 2:],\n",
    "        ccbox[:, 1],        \n",
    "        max_detections,\n",
    "        nms_iou_threshold,\n",
    "        confidence_threshold,        \n",
    "    )\n",
    "    output = tf.gather(ccbox, selected_indices)        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinaNetBoxLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Smooth L1 loss\"\"\"\n",
    "\n",
    "    def __init__(self, delta):\n",
    "        super(RetinaNetBoxLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetBoxLoss\"\n",
    "        )\n",
    "        self._delta = delta\n",
    "\n",
    "    def call(self, y_true, y_pred):    \n",
    "        \n",
    "        difference = y_true - y_pred\n",
    "        absolute_difference = tf.abs(difference)\n",
    "        squared_difference = difference ** 2\n",
    "        loss = tf.where(\n",
    "            tf.less(absolute_difference, self._delta),\n",
    "            0.5 * squared_difference,\n",
    "            absolute_difference - 0.5,\n",
    "        )\n",
    "        loss = tf.where(loss < 0.01, 0.0, loss)#new marginal loss        \n",
    "        return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "\n",
    "class RetinaNetClassificationLoss(tf.losses.Loss):\n",
    "    \"\"\"Implements Focal loss\"\"\"\n",
    "\n",
    "    def __init__(self, alpha, gamma, num_classes):\n",
    "        super(RetinaNetClassificationLoss, self).__init__(\n",
    "            reduction=\"none\", name=\"RetinaNetClassificationLoss\"\n",
    "        )\n",
    "        self._alpha = alpha\n",
    "        self._gamma = gamma\n",
    "        self._num_classes = num_classes\n",
    "\n",
    "        \n",
    "    def call(self, y_cls, y_pred):\n",
    "        y_cls = tf.cast(y_cls, dtype=tf.int32)\n",
    "        y_hot = tf.one_hot(y_cls, depth=self._num_classes, dtype=tf.float32,)\n",
    "        is_exist_non_human_class = tf.reduce_any(y_cls > 1)\n",
    "        y_positive = tf.cast(y_cls > 0+1, tf.float32)#0:bg, 1:unknown        \n",
    "        cls_weight = (y_positive + tf.cast(y_cls > 1 + 2, tf.float32)) / 2\n",
    "        dead_cls = tf.cast(tf.logical_and(y_cls > 1, y_cls < 4), tf.float32)\n",
    "        obj_weight = 1.0 - 0.5 * dead_cls\n",
    "        \n",
    "        obj_score = tf.identity(y_pred[:, :, 0], name='obj_score')\n",
    "        objectness = obj_score + tf.reduce_mean(y_pred[:, :, 1:]*0, axis=-1)\n",
    "       \n",
    "        pt = tf.nn.sigmoid(objectness)        \n",
    "        pt = tf.clip_by_value(pt, 1e-7, 1.0 - 1e-7)\n",
    "                \n",
    "        loss_p = - (1.0 - self._alpha) * tf.pow(1.0 - pt, self._gamma) * y_positive * tf.math.log(pt)        \n",
    "        loss_f = - self._alpha * tf.pow(pt, self._gamma) * (1 - y_positive) * tf.math.log(1 - pt)\n",
    "        loss_obj = loss_p + loss_f\n",
    "        \n",
    "        cls_k = len(class_names_kor)\n",
    "        y_hot = y_hot[:, :, :num_classes_real]\n",
    "        cls_pt = tf.nn.softmax(y_pred[:, :, 1:1+num_classes_real])        \n",
    "        cls_pt = tf.clip_by_value(cls_pt, 1e-7, 1.0 - 1e-7)\n",
    "        loss_cls_p = - tf.pow(1.0 - cls_pt, self._gamma) * y_hot * tf.math.log(cls_pt)\n",
    "        loss_cls_f = - tf.pow(cls_pt, self._gamma) * (1 - y_hot) * tf.math.log(1 - cls_pt)\n",
    "        loss_cls = tf.reduce_sum(loss_cls_p + loss_cls_f, axis=-1)                \n",
    "        loss = self._gamma * (obj_weight * loss_obj + 0.5 * cls_weight * loss_cls)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "class RetinaNetLoss(tf.losses.Loss):\n",
    "    \"\"\"Wrapper to combine both the losses\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=80, alpha=0.25, gamma=2.0, delta=1.0):#alpha=0.25\n",
    "        super(RetinaNetLoss, self).__init__(reduction=\"auto\", name=\"RetinaNetLoss\")\n",
    "        self._clf_loss = RetinaNetClassificationLoss(alpha, gamma, num_classes-1)\n",
    "        self._box_loss = RetinaNetBoxLoss(delta)        \n",
    "        self._num_classes = num_classes\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred : tf.concat([box_outputs, cls_outputs], axis=-1)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        \n",
    "        box_labels = y_true[:, :, :4]\n",
    "        y_cls = y_true[:, :, 4]\n",
    "        positive_max_mask = y_true[:, :, 5] > 0#new\n",
    "        \n",
    "        box_predictions = y_pred[:, :, :4]\n",
    "        h_obj = tf.nn.sigmoid(y_pred[:, :, 4])        \n",
    "        cls_predictions = y_pred[:, :, 4:-1]   \n",
    "    \n",
    "        positive_mask = tf.greater(y_cls, 0.0)\n",
    "        ignore_mask = tf.less(y_cls, 0.0)\n",
    "        \n",
    "        clf_loss = self._clf_loss(y_cls, cls_predictions)\n",
    "        box_loss = self._box_loss(box_labels, box_predictions) \n",
    "                                \n",
    "        clf_loss = tf.where(ignore_mask, 0.0, clf_loss)        \n",
    "        box_loss = tf.where(positive_mask, box_loss, 0.0)        \n",
    "        \n",
    "        max_alpha = 0.1\n",
    "        clf_loss = tf.where(positive_max_mask, (1 + max_alpha) * clf_loss, (1 - max_alpha) * clf_loss)        \n",
    "        box_loss = tf.where(positive_max_mask, (1 + max_alpha) * box_loss, (1 - max_alpha) * box_loss)\n",
    "        #box_loss = tf.where(tf.logical_and(h_obj > 0.5, positive_max_mask), box_loss, 0.0)#new    \n",
    "        \n",
    "        positive_mask = tf.cast(positive_mask, tf.float32)\n",
    "        \n",
    "        normalizer = tf.reduce_sum(positive_mask, axis=-1)                \n",
    "        clf_loss = tf.math.divide_no_nan(tf.reduce_sum(clf_loss, axis=-1), normalizer)\n",
    "        box_loss = tf.math.divide_no_nan(tf.reduce_sum(box_loss, axis=-1), normalizer)\n",
    "        \n",
    "        loss = (1 + max_alpha) * clf_loss + box_loss\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)        \n",
    "    return tp\n",
    "\n",
    "def tn(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    true_negative = tf.cast(tf.logical_and(y_cls > 0, h_postive < 1), tf.float32)\n",
    "    false_positive = (1 - y_positive) * h_postive\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tn = tf.reduce_sum(true_negative, axis=1)    \n",
    "    tn = tf.cast(tn, tf.float32)    \n",
    "    return tn\n",
    "\n",
    "def fp(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    false_positive = tf.cast(tf.logical_and(y_cls == 0, h_postive > 0), tf.float32)        \n",
    "                    \n",
    "    fp = tf.reduce_sum(false_positive, axis=1)    \n",
    "    fp = tf.cast(fp, tf.float32)    \n",
    "    return fp\n",
    "\n",
    "def fn(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)        \n",
    "    return fn\n",
    "\n",
    "def recall(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = tf.cast(y_cls > 0, tf.int32)\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]\n",
    "    h_prob = tf.nn.sigmoid(h_score)    \n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    \n",
    "    true_positives = tf.cast(tf.logical_and(y_cls > 0, h_postive>0), tf.float32)\n",
    "    false_negative = y_positive * (1 - h_postive)\n",
    "                \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)\n",
    "    fn = tf.reduce_sum(false_negative, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    \n",
    "    rec = tp / (tp + fn + 1e-8)\n",
    "    return rec\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    y_cls_symbol = tf.cast(y_true[:, :, 4], dtype=tf.int32)    \n",
    "    y_cls_symbol = tf.cast(y_cls_symbol != 0, tf.int32)\n",
    "    h_obj_prob = tf.nn.sigmoid(y_pred[:, :, 4])\n",
    "    h_cls_symbol = tf.round(h_obj_prob)    \n",
    "    h_cls_symbol = tf.cast(h_cls_symbol, tf.int32)\n",
    "    \n",
    "    true_positives = y_cls_symbol * h_cls_symbol\n",
    "    false_positive = (1 - y_cls_symbol) * h_cls_symbol\n",
    "    \n",
    "    ones = tf.ones_like(true_positives)\n",
    "    zeeros = tf.zeros_like(true_positives)\n",
    "    true_positives = tf.cast(tf.equal(true_positives, ones), tf.float32)\n",
    "    false_positive = tf.cast(tf.equal(false_positive, ones), tf.float32)\n",
    "    \n",
    "    tp = tf.reduce_sum(true_positives, axis=1)\n",
    "    fp = tf.reduce_sum(false_positive, axis=1)\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    prec = tp / (tp + fp + 1e-8)\n",
    "    return prec\n",
    "\n",
    "def accuracy(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_positive = y_cls > 0\n",
    "    y_bg = tf.cast(tf.abs(y_cls)==0, tf.int32)\n",
    "    h_score = y_pred[:, :, 4]        \n",
    "    h_prob = tf.nn.sigmoid(h_score)\n",
    "    h_postive = tf.cast(tf.round(h_prob), tf.int32)\n",
    "    h_cls = tf.math.argmax(y_pred[:, :, 5:5+num_classes_real], -1, output_type=tf.int32)        \n",
    "    acc = tf.boolean_mask(tf.equal(y_cls, h_cls), y_positive)    \n",
    "    return acc\n",
    "\n",
    "def flip_accuracy(y_true, y_pred):    \n",
    "    y_cls = tf.cast(y_true[:, :, 4], tf.int32)\n",
    "    y_flip = tf.cast(y_true[:, :, 6], tf.int32)\n",
    "    y_positive = y_cls > 0\n",
    "    h_prob = tf.nn.sigmoid(y_pred[:, :, -1])\n",
    "    h_flip = tf.cast(tf.round(h_prob), tf.int32)    \n",
    "    acc = tf.boolean_mask(tf.equal(y_flip, h_flip), y_positive)    \n",
    "    #acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "anchor_k = len(label_encoder._anchor_box.aspect_ratios)*len(label_encoder._anchor_box.scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generator():    \n",
    "    for i in range(len(input_list_train)):\n",
    "        x = input_list_train[i]\n",
    "        y_box = bbox_list_train[i]                \n",
    "        yield (x, y_box)\n",
    "\n",
    "def generator_test():    \n",
    "    for i in range(len(input_list_test)):\n",
    "        x = input_list_test[i]\n",
    "        y_box = bbox_list_test[i]        \n",
    "        yield (x, y_box)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, 5])))\n",
    "dataset_test = tf.data.Dataset.from_generator(\n",
    "    generator_test, \n",
    "    output_types=(tf.uint8, tf.float32), \n",
    "    output_shapes=(tf.TensorShape([None, None, 3]), tf.TensorShape([None, 5])))\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "for example in tfds.as_numpy(dataset):\n",
    "    image = example[0]\n",
    "    bbox = example[1]\n",
    "    print(image.dtype, image.shape, bbox.shape, bbox[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #finetune Freeze 32, 4 ok, 5 Warn\n",
    "autotune = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.shuffle(32 * batch_size)\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#train_dataset = train_dataset.padded_batch(batch_size=batch_size)\n",
    "train_dataset = train_dataset.map(label_encoder.encode_batch_train, num_parallel_calls=autotune)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "train_dataset = train_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = dataset_test.map(preprocess_test_data, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.padded_batch(batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=False)\n",
    "#val_dataset = val_dataset.padded_batch(batch_size=batch_size)\n",
    "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "val_dataset = val_dataset.prefetch(autotune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=200)\n",
    "image_height, image_width = padded_image_shape\n",
    "\n",
    "img_check = 0\n",
    "for image, output_map in train_dataset:\n",
    "    print('output_map', output_map.shape)\n",
    "    cbbox = output_map    \n",
    "    bbox = cbbox[:, :, :4]\n",
    "    cls_gt = cbbox[:,:,4]\n",
    "    img_m, image_height, image_width, image_ch = image.shape\n",
    "    anchor_feature_size = [(np.ceil(image_height / 2 ** i), np.ceil(image_width / 2 ** i)) \n",
    "                           for i in range(level_start, level_end)]\n",
    "    print('anchor_feature_size', anchor_feature_size)    \n",
    "    m = len(cbbox)    \n",
    "    positive_count = np.sum(cls_gt>0)\n",
    "    print('cbbox', cbbox.shape)\n",
    "    print('cls_sum',np.sum(cls_gt < 0.0), np.sum(cls_gt == 0.0), \n",
    "          np.sum(cls_gt == 1.0), np.sum(cls_gt > 1.0))\n",
    "    print('cls_mean',np.mean(cls_gt < 0.0), np.mean(cls_gt == 0.0), \n",
    "          np.mean(cls_gt == 1.0), np.mean(cls_gt > 0.0))\n",
    "    print('shape',image.shape, cbbox.shape,'unique', np.unique(cls_gt))\n",
    "    print('anchor_feature_size', anchor_feature_size)\n",
    "    offset = 0\n",
    "    positive_maps = []\n",
    "    for anchor_feature_size_1 in anchor_feature_size:        \n",
    "        fm_h, fm_w = anchor_feature_size_1\n",
    "        fm_h = int(fm_h)\n",
    "        fm_w = int(fm_w)        \n",
    "        fm_wh = int(fm_h * fm_w * anchor_k)\n",
    "        cbbox_anchor = cbbox[:, offset:offset+fm_wh, 4]\n",
    "        cbbox_anchor = np.reshape(cbbox_anchor, [m, fm_h, fm_w, anchor_k])\n",
    "        coount_m1 = np.count_nonzero(cbbox_anchor==-1)\n",
    "        coount_0 = np.count_nonzero(cbbox_anchor==0)\n",
    "        coount_1 = np.count_nonzero(cbbox_anchor==1)\n",
    "        coount_1_over = np.count_nonzero(cbbox_anchor>1)\n",
    "        positive_ratio = np.mean(cbbox_anchor>0)\n",
    "        positive_maps.append(cbbox_anchor>0)\n",
    "        print('cbbox_anchor', cbbox_anchor.shape, coount_m1, coount_0, coount_1, coount_1_over, 'ratio', positive_ratio)\n",
    "        sample_0_cbbox = cbbox_anchor[0]\n",
    "        sample_0_cbbox_sum = np.max(sample_0_cbbox, -1).astype(np.int)       \n",
    "      \n",
    "        offset += fm_wh\n",
    "        if False:            \n",
    "            file_name = str(fm_h)+ '_' + str(fm_w)+ '.txt'\n",
    "            np.savetxt(file_name,sample_0_cbbox_sum, fmt='%d',delimiter='')\n",
    "    img_check = image\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.max(positive_maps[0][0], -1))\n",
    "plt.title(str(positive_maps[0].shape)+ str(np.mean(positive_maps[0][0]))+ ' ' + str(np.sum(positive_maps[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmap0 = np.array(Image.fromarray(np.max(positive_maps[0][0],-1)).resize((image_width, image_height)))\n",
    "pmap1 = np.array(Image.fromarray(np.max(positive_maps[1][0],-1)).resize((image_width, image_height)))\n",
    "pmap2 = np.array(Image.fromarray(np.max(positive_maps[2][0],-1)).resize((image_width, image_height)))\n",
    "pmap3 = np.array(Image.fromarray(np.max(positive_maps[3][0],-1)).resize((image_width, image_height)))\n",
    "pmap4 = np.array(Image.fromarray(np.max(positive_maps[4][0],-1)).resize((image_width, image_height)))\n",
    "pmap0 = pmap0.astype(np.uint8)\n",
    "pmap1 = pmap1.astype(np.uint8)\n",
    "pmap2 = pmap2.astype(np.uint8)\n",
    "pmap3 = pmap3.astype(np.uint8)\n",
    "pmap4 = pmap4.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pmap_with_img = np.array(img_check)[0]#*255\n",
    "pmap_with_img = pmap_with_img.astype(np.uint8)\n",
    "pmap_add = np.expand_dims(pmap0+pmap1+pmap2+pmap3+pmap4, -1)\n",
    "pmap = (pmap_add>0).astype(np.uint8)*255\n",
    "mix_rgb = np.concatenate((pmap, pmap_with_img[:,:,1:]),-1)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(mix_rgb)\n",
    "plt.title(str(np.mean(pmap_add)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight():   \n",
    "    weights_dir = path_weight#\"data\"\n",
    "    #latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
    "    latest_checkpoint = weights_dir \n",
    "    print('latest_checkpoint', latest_checkpoint)\n",
    "    model.load_weights(weights_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = tf.optimizers.SGD(learning_rate=1e-4, momentum=0.1, clipvalue=5.)#warm up clipvalue=10. !\n",
    "optimizer = tf.optimizers.SGD(learning_rate=1e-4, momentum=0.1, clipvalue=10.)\n",
    "loss_detect = RetinaNetLoss(num_classes)\n",
    "model = createRetinaNet(num_classes, anchor_k)\n",
    "#metrics = [recall, precision, accuracy]\n",
    "metrics = [recall, precision, accuracy, tp, tn, fp, fn]\n",
    "model.compile(loss=loss_detect, optimizer=optimizer, metrics=metrics)#[recall, precision, accuracy]\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=path_weight,\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=True,\n",
    "        verbose=0,\n",
    "        save_freq=200\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_list_train), len(input_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "before   3s 32ms/step - loss: 5.6431 - recall: 0.8978 - precision: 0.9538 - accuracy: 0.4959 - tp: 43.4000 - tn: 4.3300 - fp: 10.4600 - fn: 4.3300\n",
    "after  101s 31ms/step - loss: 0.9343 - recall: 0.9220 - precision: 0.9705 - accuracy: 0.9624 - tp: 81.0678 - tn: 5.3810 - fp: 8.1468 - fn: 5.3810\n",
    "        97s 30ms/step - loss: 0.7515 - recall: 0.9336 - precision: 0.9723 - accuracy: 0.9813 - tp: 82.3051 - tn: 4.1437 - fp: 7.8271 - fn: 4.1437\n",
    "'''\n",
    "out = model.evaluate(val_dataset.take(10000))\n",
    "#out = model.evaluate(train_dataset.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=None,#val_dataset.take(2)\n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks_list,#callbacks_list\n",
    "    verbose=1,\n",
    ")\n",
    "'''\n",
    "effD2 Freeze:652ms/step - loss: 0.8893 - recall: 0.9685 - precision: 0.9854 - accuracy: 0.9676 - flip_accuracy: 0.9951\n",
    "effD4 fine  : 1926s 340ms/step - loss: 4.3644 - recall: 0.4212 - precision: 0.7165 - accuracy: 0.3599 - tp: 56.8611\n",
    "      fine  : 1879s 310ms/step - loss: 3.8263 - recall: 0.3655 - precision: 0.6898 - accuracy: 0.4074 - tp: 18.4866\n",
    "      fine  :  395s 348ms/step - loss: 0.6502 - recall: 0.9364 - precision: 0.9775 - accuracy: 0.9709\n",
    "               770s 468ms/step - loss: 0.8848 - recall: 0.9223 - precision: 0.9692 - accuracy: 0.9713 - tp: 78.4045 - tn: 5.4041 - fp: 7.2930 - fn: 5.4041\n",
    "     rotate : 2875s 476ms/step - loss: 0.6503 - recall: 0.9334 - precision: 0.9748 - accuracy: 0.9814 - tp: 59.4221 - tn: 3.5259 - fp: 3.6282 - fn: 3.5259\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(path_weight)\n",
    "path_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.keras.Input(shape=[padded_image_shape[0], padded_image_shape[1], 3], name=\"image\")\n",
    "predictions = model(image, training=False)\n",
    "\n",
    "detections = decodePredictions(image, predictions, confidence_threshold=0.5, nms_iou_threshold=0.1)\n",
    "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(12, 10), linewidth=1, color=[0, 0, 1], \n",
    "    boxes_gt=None):\n",
    "    \n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = attach_crop_image(image, boxes, max_crop=200)        \n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()        \n",
    "   \n",
    "    if boxes_gt is not None:\n",
    "        for box in boxes_gt:        \n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            patch = plt.Rectangle(\n",
    "                [x1, y1], w, h, fill=False, edgecolor=[0,1,0], linewidth=2\n",
    "            )\n",
    "            ax.add_patch(patch)\n",
    "            \n",
    "    for box, cls, score in zip(boxes, classes, scores):        \n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        color_text = edgecolors[cls]\n",
    "        color = [0, 0, 1]\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        if len(boxes) < 100:\n",
    "            \n",
    "            cls_name = class_names[cls] if cls < len(class_names) else 'unknown'\n",
    "            score_txt = str.format('%s %.2f' %(cls_name, score))\n",
    "            ax.text(x1, y1+20, score_txt, bbox={\"facecolor\": color_text, \"alpha\": 0.4}, clip_box=ax.clipbox, clip_on=True,)\n",
    "          \n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_test(test_datas, bboxs_label, step=1, is_show=True):\n",
    "    list_h = []\n",
    "    list_y = []\n",
    "    i = 0\n",
    "    for image, cbbox in test_datas: \n",
    "        if i%step==0:            \n",
    "            bbox_annotation = bboxs_label[i]\n",
    "            scale = np.array(image.shape[:2])[::-1]\n",
    "            scale = np.reshape(scale, [1, 2])\n",
    "            scale = np.concatenate((scale, scale), 1)\n",
    "            gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "               \n",
    "            input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "            input_image = tf.expand_dims(input_image, axis=0)\n",
    "            #input_image = tf.cast(input_image, tf.uint8)\n",
    "            detected_box = inference_model.predict(input_image)        \n",
    "            print(input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio)\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "            list_h.append(np.concatenate((np.expand_dims(cls_h,-1), box), -1))\n",
    "            list_y.append(np.concatenate((bbox_annotation[:, :1], gt_bbox), -1))\n",
    "            if is_show:\n",
    "                visualize_detections(\n",
    "                    image,\n",
    "                    box,\n",
    "                    cls_h,\n",
    "                    scores,\n",
    "                    boxes_gt=gt_bbox\n",
    "                )\n",
    "        i+=1\n",
    "        \n",
    "    return list_h, list_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_h, list_y = check_test(dataset_test, bbox_list_test, step=1, is_show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_test(dataset, bbox_list_train, step=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(class_names_kor), np.array(class_names_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xywh(box):#`[x, y, width, height] to corner\n",
    "    \n",
    "    xywh = np.concatenate([box[:2] - box[2:] / 2.0, box[:2] + box[2:] / 2.0], 0)\n",
    "    xywh = np.reshape(xywh, (-1))\n",
    "    return xywh\n",
    "         \n",
    "def iou(box1, box2):\n",
    "   \n",
    "    box1_corner = convert_xywh(box1)\n",
    "    box2_corner = convert_xywh(box2)\n",
    "        \n",
    "    lu = np.maximum(box1_corner[:2], box2_corner[:2])\n",
    "    rd = np.minimum(box1_corner[2:], box2_corner[2:])#todo update\n",
    "    intersection = np.maximum(0.0, rd - lu)\n",
    "    intersection_area = intersection[0] * intersection[1]\n",
    "    boxes1_area = box1[2] * box1[3]\n",
    "    boxes2_area = box2[2] * box2[3]\n",
    "   \n",
    "    union_area = np.maximum(\n",
    "        boxes1_area + boxes2_area - intersection_area, 1e-8\n",
    "    )    \n",
    "    #print('intersection_area', intersection_area, union_area)\n",
    "    v = np.clip(intersection_area / union_area, 0.0, 1.0)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "positives = np.zeros(len(class_names_kor), np.int)\n",
    "trues = np.zeros_like(positives)\n",
    "tp = np.zeros_like(positives)\n",
    "\n",
    "_list_h = copy.deepcopy(list_h)\n",
    "_list_y = copy.deepcopy(list_y)\n",
    "for H, Y in zip(_list_h, _list_y):\n",
    "         \n",
    "    for h in H:\n",
    "        h_cls = int(h[0])            \n",
    "        trues[h_cls] += 1\n",
    "        \n",
    "    for y in Y:        \n",
    "        \n",
    "        y_cls = int(y[0])\n",
    "        positives[y_cls] += 1\n",
    "        \n",
    "        for h in H:            \n",
    "            h_cls = int(h[0]) \n",
    "            io = iou(y[1:], h[1:]) \n",
    "            if io > 0.5 and (h_cls == y_cls):\n",
    "                #print('io', io)\n",
    "                tp[y_cls] += 1\n",
    "                y[0] = h[0] = -1\n",
    "                #print('iou', io)\n",
    "                break\n",
    "        \n",
    "print('positives', positives)\n",
    "print('trues    ', trues)\n",
    "print('tp       ', tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('positives', [p for p in positives])\n",
    "print('trues    ', [p for p in trues])\n",
    "print('tp       ', [p for p in tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives [   0   82 1318 2009    0    0 6358  522  310   48   11   12   13  372  309    1  172    0    0    0    0]\n",
    "trues     [   0    0 1217 1854    0    0 6275  544  250   23    0   14    8  358  339    0  167    0    1    0    0]\n",
    "tp        [   0    0 1170 1674    0    0 5854  366  164   11    0   11    8  343  286    0  139    0    0    0    0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_kor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save('./pig_efficientDet-D2_save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Convert Keras model to ConcreteFunction\n",
    "full_model = tf.function(lambda x: inference_model(x))\n",
    "full_model = full_model.get_concrete_function(\n",
    "    x=tf.TensorSpec(inference_model.inputs[0].shape, inference_model.inputs[0].dtype))\n",
    "\n",
    "# Get frozen ConcreteFunction\n",
    "frozen_func = convert_variables_to_constants_v2(full_model)\n",
    "frozen_func.graph.as_graph_def()\n",
    "\n",
    "layers = [op.name for op in frozen_func.graph.get_operations()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(frozen_func.outputs))\n",
    "frozen_list = frozen_func.outputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))\n",
    "\n",
    "print(type(frozen_func.inputs))\n",
    "frozen_list = frozen_func.inputs\n",
    "print(frozen_list)\n",
    "print(len(frozen_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.io.write_graph(graph_or_graph_def=frozen_func.graph,\n",
    "                  logdir=\"./frozen_models\",\n",
    "                  name=\"pig_EfficientDet-D2_frozen_graph.pb\",\n",
    "                  as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(X, Y, Path, step=1):\n",
    "    \n",
    "    for i in range(len(X)): \n",
    "        image = X[i]\n",
    "        bbox_annotation = Y[i]\n",
    "        path = Path[i]\n",
    "        \n",
    "        scale = np.array(image.shape[:2])[::-1]\n",
    "        scale = np.reshape(scale, [1, 2])\n",
    "        scale = np.concatenate((scale, scale), 1)\n",
    "        gt_bbox = bbox_annotation[:, 1:] * scale\n",
    "\n",
    "        input_image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
    "        input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "        detected_box = inference_model.predict(input_image)\n",
    "        detect_k = len(detected_box)\n",
    "        if detect_k!= len(bbox_annotation):\n",
    "            print(path, input_image.shape, 'detected_box', detected_box.shape, scale, 'ratio',ratio.numpy())\n",
    "            #(1, 1024, 1920, 3) detected_box (7, 6) [[1920 1080 1920 1080]] ratio tf.Tensor(0.94814813, shape=(), dtype=float32)\n",
    "            cls_h = detected_box[:, 0].astype(np.int)\n",
    "            scores = detected_box[:, 1]\n",
    "            box = detected_box[:, 2:] / ratio\n",
    "            #print('box', box)\n",
    "\n",
    "            visualize_detections(\n",
    "                image,\n",
    "                box,\n",
    "                cls_h,\n",
    "                scores,\n",
    "                boxes_gt=gt_bbox\n",
    "            )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = start + 100\n",
    "check_error(input_list[start:end], bbox_list[start:end], path_list[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model from .pb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_pb = './pig_pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.save(saved_model_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {'recall':recall,'precision':precision}\n",
    "model_loaded = keras.models.load_model(saved_model_pb, custom_objects=custom_objects, compile=False)\n",
    "#model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image, cbbox in val_dataset: \n",
    "    \n",
    "    detected_box = model_loaded.predict(image)\n",
    "    print('detected_box', detected_box.shape)\n",
    "    if len(detected_box) > 0:\n",
    "        cls_h = detected_box[:, 0].astype(np.int)\n",
    "        scores = detected_box[:, 1]\n",
    "        box = detected_box[:, 2:]\n",
    "\n",
    "        visualize_detections(\n",
    "            image[0],\n",
    "            box,\n",
    "            cls_h,\n",
    "            scores\n",
    "        )    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
